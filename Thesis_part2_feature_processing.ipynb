{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WupZzSis5tYI"
      },
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi\n",
        "\n",
        "\n",
        "!pip install imbalanced-learn\n",
        "\n",
        "# Install RAPIDS\n",
        "!pip install --upgrade pip\n",
        "!pip install cudf-cu11 cuml-cu11 --extra-index-url=https://pypi.ngc.nvidia.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmPjYCCdQZp8"
      },
      "outputs": [],
      "source": [
        "# Data manipulation and visualization\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# File and OS operations\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Google Colab files (if using Colab)\n",
        "from google.colab import files\n",
        "\n",
        "# Warnings and display\n",
        "import warnings\n",
        "from IPython.display import display\n",
        "\n",
        "# Scipy statistics\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Scikit-learn modules\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import (\n",
        "    GridSearchCV,\n",
        "    StratifiedKFold,\n",
        "    train_test_split,\n",
        "    learning_curve,\n",
        "    cross_val_score,\n",
        "    cross_val_predict\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    precision_recall_curve,\n",
        "    average_precision_score,\n",
        "    accuracy_score,\n",
        "    log_loss,\n",
        "    f1_score,\n",
        "    mean_squared_error,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    make_scorer\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFECV, SelectFromModel\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Imbalanced-learn modules\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# Joblib for model persistence\n",
        "import joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import __version__ as sklearn_version\n",
        "print(\"scikit‑learn version:\", sklearn_version)\n"
      ],
      "metadata": {
        "id": "McCQl5sNAdo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiLrbaZure3b"
      },
      "source": [
        "Code to fuse data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBonh4ozG4DE"
      },
      "outputs": [],
      "source": [
        "uploaded_with_prep = files.upload()\n",
        "\n",
        "fused_with_prep = pd.DataFrame()\n",
        "\n",
        "for filename, content in uploaded_with_prep.items():\n",
        "    temp_df = pd.read_csv(filename)\n",
        "    fused_with_prep = pd.concat([fused_with_prep, temp_df], ignore_index=True)\n",
        "\n",
        "# Save and download fused DataFrames\n",
        "fused_with_prep.to_csv('extracted_features_with_prep.csv', index=False)\n",
        "\n",
        "print(\"Fused files created:\")\n",
        "#files.download('extracted_features_with_prep.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3U1iYWzr8fI"
      },
      "source": [
        "Visualize data and Clean Names and Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rESF65h5Umlc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"extracted_features_with_prep.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Fix Names of Features\n",
        "df.columns = [col.replace(\"\\n \", \" \").strip() for col in df.columns]\n",
        "\n",
        "# Save the Cleaner dataset\n",
        "df.to_csv(\"extracted_features_with_prep.csv\", index=False)\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "df.hist(bins=30, figsize=(20, 15), edgecolor='black')\n",
        "plt.suptitle(\"Feature Distributions\", fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUuprwWRm3Lw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set a clean style\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# 1) Count Plot for Gender\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(\n",
        "    data=df,\n",
        "    x='Gender',\n",
        "    palette='Set2',\n",
        "    edgecolor='black',\n",
        "    linewidth=0.5\n",
        ")\n",
        "plt.title(\"Gender Distribution\")\n",
        "plt.xticks([0, 1], [\"Male (1)\", \"Female (2)\"])\n",
        "plt.xlabel(\"Gender\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# 2) Histogram for Age\n",
        "min_age = df['Age'].min()\n",
        "max_age = df['Age'].max()\n",
        "bins = np.arange(min_age - 0.5, max_age + 0.5 + 1, 1)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.histplot(\n",
        "    data=df,\n",
        "    x='Age',\n",
        "    bins = np.arange(min_age - 0.5, max_age + 0.5 + 1, 1),\n",
        "    palette='Set2',\n",
        "    edgecolor='black',\n",
        "    linewidth=0.5\n",
        ")\n",
        "plt.title(\"Age Distribution\")\n",
        "plt.xticks(np.arange(min_age, max_age + 1, 1))\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcGZ3agwZCK_"
      },
      "source": [
        "Sort the left vs right status sorting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "der2ufoIZG48"
      },
      "outputs": [],
      "source": [
        "# Function to update statuses based on filename and status\n",
        "def update_status(df):\n",
        "    for index, row in df.iterrows():\n",
        "        filename = row['Filename']\n",
        "        status = row['Status']\n",
        "\n",
        "        # Ensure filename is long enough to access the 8th character\n",
        "        if len(filename) >= 8:\n",
        "            eighth_char = filename[7]  # Python indexing starts at 0\n",
        "\n",
        "            # Apply conditions based on the rules\n",
        "            if status == 1:\n",
        "                if eighth_char == 'R':\n",
        "                    df.at[index, 'Status'] = 0\n",
        "                elif eighth_char == 'L':\n",
        "                    df.at[index, 'Status'] = 1\n",
        "            elif status == 2:\n",
        "                if eighth_char == 'R':\n",
        "                    df.at[index, 'Status'] = 1\n",
        "                elif eighth_char == 'L':\n",
        "                    df.at[index, 'Status'] = 0\n",
        "            elif status == 3:\n",
        "                if eighth_char == 'R' or eighth_char == 'L':\n",
        "                    df.at[index, 'Status'] = 1\n",
        "    return df\n",
        "\n",
        "\n",
        "# Function to update measurement columns based on the leg indicated in the filename\n",
        "def update_measurements(df):\n",
        "    def select_measurements(row):\n",
        "        filename = row['Filename']\n",
        "        if len(filename) >= 8:\n",
        "            leg = filename[7]  # 8th character: 'L' or 'R'\n",
        "        else:\n",
        "            leg = None\n",
        "\n",
        "        if leg == 'L':\n",
        "            return pd.Series({\n",
        "                \"Leg Length\": row[\"L Leg Length\"],\n",
        "                \"Q-Angle\": row[\"L Q-Angle\"],\n",
        "                \"Active Knee\": row[\"L Active Knee Flexion Angle\"]\n",
        "            })\n",
        "        elif leg == 'R':\n",
        "            return pd.Series({\n",
        "                \"Leg Length\": row[\"R Leg Length\"],\n",
        "                \"Q-Angle\": row[\"R Q-Angle\"],\n",
        "                \"Active Knee\": row[\"R Active Knee Flexion Angle\"]\n",
        "            })\n",
        "        else:\n",
        "            return pd.Series({\"Leg Length\": None, \"Q-Angle\": None, \"Active Knee\": None})\n",
        "\n",
        "    # Apply row-wise selection and create new columns\n",
        "    df[['Leg Length', 'Q-Angle', 'Active Knee']] = df.apply(select_measurements, axis=1)\n",
        "\n",
        "    # Drop the original left/right specific measurement columns\n",
        "    df.drop(columns=['L Leg Length', 'R Leg Length', 'L Q-Angle', 'R Q-Angle',\n",
        "                     'L Active Knee Flexion Angle', 'R Active Knee Flexion Angle',], inplace=True)\n",
        "    return df\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"extracted_features_with_prep.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "update_status(df)\n",
        "update_measurements(df)\n",
        "\n",
        "# Save the cleaned dataset\n",
        "df.to_csv(\"processed_features.csv\", index=False)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "df.hist(bins=30, figsize=(20, 15), edgecolor='black')\n",
        "\n",
        "plt.suptitle(\"Feature Distributions\", fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Overwrite the existing CSV file with the modified DataFrame\n",
        "df.to_csv(\"extracted_features_with_prep.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0WIPHxQGrt8"
      },
      "source": [
        "drop code number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOgZljmbFszN"
      },
      "outputs": [],
      "source": [
        "df.drop('Code_Number_6digits', axis=1, inplace=True)\n",
        "df.to_csv(\"extracted_features_with_prep.csv\", index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKzd8vqi0CgL"
      },
      "outputs": [],
      "source": [
        "files.download('extracted_features_with_prep.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LUf4N9E57aSR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"extracted_features_with_prep.csv\"\n",
        "df = pd.read_csv(output_combined_file)\n",
        "\n",
        "# Create the boxplot on its own Axes\n",
        "fig, ax = plt.subplots(figsize=(30, 10))\n",
        "sns.boxplot(data=df, whis=1.5, width=0.5, ax=ax)\n",
        "\n",
        "# Customize titles and tick labels\n",
        "ax.set_title(\"Feature Distributions\", fontsize=16)\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "# Make layout tight so labels and boxes don't overlap\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# (Optional) Overwrite the CSV if you've made in‑memory modifications\n",
        "df.to_csv(file_path, index=False)\n",
        "\n",
        "# Print out the column names\n",
        "print(\"Columns in DataFrame:\", df.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQAW_Atv81sM"
      },
      "outputs": [],
      "source": [
        "# Define the features to plot against \"Status\"\n",
        "features = [\n",
        "      'Mean', 'Std', 'Variance', 'Skewness', 'Kurtosis',\n",
        "       'Dissimilarity_0', 'Dissimilarity_45', 'Dissimilarity_90',\n",
        "       'Dissimilarity_135', 'Correlation_0', 'Correlation_45',\n",
        "       'Correlation_90', 'Correlation_135', 'Homogeneity_0', 'Homogeneity_45',\n",
        "       'Homogeneity_90', 'Homogeneity_135', 'Energy_0', 'Energy_45',\n",
        "       'Energy_90', 'Energy_135', 'Contrast_0', 'Contrast_45', 'Contrast_90',\n",
        "       'Contrast_135', 'RunLengthNonUniformity', 'ShortRunEmphasis',\n",
        "       'LongRunEmphasis', 'GrayLevelNonUniformity', 'RunPercentage',\n",
        "       'RunEntropy', 'Area', 'Eccentricity', 'Perimeter', 'Centroid_X',\n",
        "       'Centroid_Y', 'Hu0',\t'Hu1',\t'Hu2',\t'Hu3',\t'Hu4',\t'Hu5',\t'Hu6', 'Age',\n",
        "       'Gender', 'Active Playing Years', 'Height', 'Weight',\n",
        "       'Leg Length', 'Q-Angle', 'Active Knee'\n",
        "            ]\n",
        "\n",
        "# Function to create scatter plots for all features in a single figure\n",
        "def plot_scatter_grid(scaled_df, title):\n",
        "    num_features = len(features)\n",
        "    num_cols = 5\n",
        "    num_rows = (num_features // num_cols) + (num_features % num_cols > 0)  # Calculate rows\n",
        "\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 15))\n",
        "    fig.suptitle(title, fontsize=18)\n",
        "\n",
        "    axes = axes.flatten()  # For easy iteration\n",
        "\n",
        "    for i, feature in enumerate(features):\n",
        "        sns.scatterplot(ax=axes[i], data=scaled_df, x=feature, y='Status', alpha=0.5)\n",
        "        axes[i].set_title(feature)\n",
        "        axes[i].set_xlabel(\"\")\n",
        "        axes[i].set_ylabel(\"Status\")\n",
        "\n",
        "    # Hide any empty subplots\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to fit title\n",
        "    plt.show()\n",
        "\n",
        "plot_scatter_grid(df, \"Raw Data - Scatter Plots\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH2_H_qU5mj8"
      },
      "source": [
        "Initial Normalization for viewing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gt3ra-nh5oqw"
      },
      "outputs": [],
      "source": [
        "# Load the uploaded CSV file into a DataFrame\n",
        "df_with_prep = pd.read_csv('extracted_features_with_prep.csv')\n",
        "\n",
        "\n",
        "def normalize_features_min_max(df):\n",
        "    normalized_df = df.copy()\n",
        "    for column in normalized_df.columns:\n",
        "        # Skip non-numeric columns\n",
        "        if column in ['Filename', 'Status', 'Gender']:\n",
        "            continue\n",
        "\n",
        "        # Check if the column is numeric\n",
        "        if pd.api.types.is_numeric_dtype(normalized_df[column]):\n",
        "            min_val = normalized_df[column].min()\n",
        "            max_val = normalized_df[column].max()\n",
        "\n",
        "            # Avoid division by zero (constant column)\n",
        "            if max_val != min_val:\n",
        "                normalized_df[column] = (normalized_df[column] - min_val) / (max_val - min_val)\n",
        "            else:\n",
        "                print(f\"Warning: Column '{column}' has constant values. Skipping normalization.\")\n",
        "    return normalized_df\n",
        "\n",
        "\n",
        "df_with_prep_normalized = normalize_features_min_max(df_with_prep)\n",
        "\n",
        "\n",
        "df_with_prep_normalized.to_csv('Features_With_Preprocessing_Normalized.csv', index=False)\n",
        "print(\"Normalized 'with_prep' file saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spearman methods"
      ],
      "metadata": {
        "id": "3B5Wrr5v5iTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Load and clean data.\n",
        "df = pd.read_csv(\"extracted_features_with_prep.csv\")\n",
        "df = df.drop(columns=[col for col in df.columns if \"Code_Number_6digits\" in col])\n",
        "#df = df.drop(columns=[\"Filename\"])\n",
        "\n",
        "# Extract features and target.\n",
        "X = df.drop(columns=[\"Status\"]).values\n",
        "y = df[\"Status\"].values\n",
        "\n",
        "# Compute Spearman correlation between rows (feature vectors)\n",
        "rho_matrix, _ = spearmanr(X, axis=1)\n",
        "\n",
        "# Separate correlations into intra-class and inter-class\n",
        "classes = np.unique(y)\n",
        "intra_class_corrs = []\n",
        "inter_class_corrs = []\n",
        "\n",
        "# Gather intra-class correlations\n",
        "for c in classes:\n",
        "    indices = np.where(y == c)[0]\n",
        "    intra_corr_matrix = rho_matrix[np.ix_(indices, indices)]\n",
        "    intra_class_corrs.extend(intra_corr_matrix[np.triu_indices(len(indices), k=1)])\n",
        "\n",
        "# Gather inter-class correlations\n",
        "for i in range(len(classes)):\n",
        "    for j in range(i+1, len(classes)):\n",
        "        indices_i = np.where(y == classes[i])[0]\n",
        "        indices_j = np.where(y == classes[j])[0]\n",
        "        inter_corr_matrix = rho_matrix[np.ix_(indices_i, indices_j)]\n",
        "        inter_class_corrs.extend(inter_corr_matrix.ravel())\n",
        "\n",
        "# Visualization of correlations\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Histogram visualization\n",
        "plt.hist(intra_class_corrs, bins=30, alpha=0.6, label=\"Intra-class\", density=True, color='blue')\n",
        "plt.hist(inter_class_corrs, bins=30, alpha=0.6, label=\"Inter-class\", density=True, color='orange')\n",
        "plt.title(\"Spearman Correlation Distribution (Feature Vectors)\")\n",
        "plt.xlabel(\"Spearman Correlation\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Boxplot visualization\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.boxplot(data=[intra_class_corrs, inter_class_corrs], palette=['blue', 'orange'])\n",
        "plt.xticks([0, 1], ['Intra-class', 'Inter-class'])\n",
        "plt.title(\"Boxplot of Spearman Correlation Distributions\")\n",
        "plt.ylabel(\"Spearman Correlation\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ohZAgD2u5fWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Load and clean data\n",
        "df = pd.read_csv(\"extracted_features_with_prep.csv\")\n",
        "df = df.drop(columns=[col for col in df.columns if \"Code_Number_6digits\" in col])\n",
        "df = df.drop(columns=[\"Filename\"])\n",
        "X = df.drop(columns=[\"Status\"])\n",
        "y = df[\"Status\"].values\n",
        "classes = np.unique(y)\n",
        "\n",
        "# Loop over each feature\n",
        "for feature in X.columns:\n",
        "    feature_vals = X[feature].values\n",
        "\n",
        "    # Compute Spearman correlation between samples (via ranks + Pearson)\n",
        "    corr_matrix, _ = spearmanr(np.expand_dims(feature_vals, axis=1), axis=1)\n",
        "\n",
        "    # Safeguard in case it returns a float (shouldn't happen here, but safe)\n",
        "    if isinstance(corr_matrix, float):\n",
        "        corr_matrix = np.ones((len(feature_vals), len(feature_vals)))\n",
        "\n",
        "    intra_feature_corrs = []\n",
        "    inter_feature_corrs = []\n",
        "\n",
        "    for c in classes:\n",
        "        indices = np.where(y == c)[0]\n",
        "        intra = corr_matrix[np.ix_(indices, indices)]\n",
        "        intra_feature_corrs.extend(intra[np.triu_indices(len(indices), k=1)])\n",
        "\n",
        "    for i in range(len(classes)):\n",
        "        for j in range(i + 1, len(classes)):\n",
        "            idx_i = np.where(y == classes[i])[0]\n",
        "            idx_j = np.where(y == classes[j])[0]\n",
        "            inter = corr_matrix[np.ix_(idx_i, idx_j)]\n",
        "            inter_feature_corrs.extend(inter.ravel())\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.hist(intra_feature_corrs, bins=25, alpha=0.6, label=\"Intra-class\", density=True, color='green')\n",
        "    plt.hist(inter_feature_corrs, bins=25, alpha=0.6, label=\"Inter-class\", density=True, color='red')\n",
        "    plt.title(f\"Spearman Correlation Distribution for Feature '{feature}'\")\n",
        "    plt.xlabel(\"Spearman Correlation\")\n",
        "    plt.ylabel(\"Density\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "DITfwQSIO2wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ams8snls7VYl"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"extracted_features_with_prep.csv\")\n",
        "df = df.drop(columns=[col for col in df.columns if \"Code_Number_6digits\" in col])\n",
        "df = df.drop(columns=[col for col in df.columns if \"Filename\" in col])\n",
        "\n",
        "# Separate features and target.\n",
        "features = df.drop(columns=[\"Status\"])\n",
        "target = df[\"Status\"]\n",
        "\n",
        "# Split the data into separate DataFrames for each class.\n",
        "df_class0 = df[df[\"Status\"] == 0].drop(columns=[\"Status\"])\n",
        "df_class1 = df[df[\"Status\"] == 1].drop(columns=[\"Status\"])\n",
        "\n",
        "# Compute the correlation matrix for each class.\n",
        "corr_class0 = df_class0.corr(method='spearman')\n",
        "corr_class1 = df_class1.corr(method='spearman')\n",
        "\n",
        "\n",
        "# Plot the correlation matrix for Class 0\n",
        "plt.figure(figsize=(20, 16))\n",
        "sns.heatmap(corr_class0, annot=False, cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation Matrix for Class 0\", fontsize=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot the correlation matrix for Class 1\n",
        "plt.figure(figsize=(20, 16))\n",
        "sns.heatmap(corr_class1, annot=False, cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation Matrix for Class 1\", fontsize=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compute the difference\n",
        "diff_corr = corr_class0 - corr_class1\n",
        "\n",
        "# Plot the difference in correlations\n",
        "plt.figure(figsize=(20, 16))\n",
        "sns.heatmap(diff_corr, annot=False, cmap=\"coolwarm\")\n",
        "plt.title(\"Difference in Correlation (Class 0 - Class 1)\", fontsize=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4mxCCcv-Dnz"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('Features_With_Preprocessing_Normalized.csv')\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "df.hist(bins=30, figsize=(20, 15), edgecolor='black')\n",
        "plt.suptitle(\"Feature Distributions after Normalization\", fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMhezwmX7Ujr"
      },
      "source": [
        "Prep for Visulaization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nx-G--aM7T2p"
      },
      "outputs": [],
      "source": [
        "with_prep_file = 'Features_With_Preprocessing_Normalized.csv'\n",
        "output_combined_file = 'combined_all_features_interleaved.csv'\n",
        "\n",
        "with_prep_df = pd.read_csv(with_prep_file)\n",
        "with_prep_df = with_prep_df.drop( columns=[col for col in with_prep_df.columns if \"Code_Number_6digits\" in col])\n",
        "\n",
        "\n",
        "\n",
        "# Ensure the 'Status' column is numeric\n",
        "with_prep_df['Status'] = pd.to_numeric(with_prep_df['Status'], errors='coerce')\n",
        "\n",
        "# Drop rows with NaN in 'Status' if any exist after conversion\n",
        "with_prep_df = with_prep_df.dropna(subset=['Status'])\n",
        "\n",
        "# Separate data based on `Status`\n",
        "status_1_data = with_prep_df[with_prep_df['Status'] == 1].drop(columns=['Status'])\n",
        "status_0_data = with_prep_df[with_prep_df['Status'] == 0].drop(columns=['Status'])\n",
        "\n",
        "# Interleave all features between `Status 1` and `Status 0`\n",
        "interleaved_columns = []\n",
        "for col in with_prep_df.columns:\n",
        "    if col not in ['Filename', 'Status']:  # Exclude non-feature columns\n",
        "        if col in status_1_data.columns:\n",
        "            interleaved_columns.append(f\"{col}_status_1\")\n",
        "        if col in status_0_data.columns:\n",
        "            interleaved_columns.append(f\"{col}_status_0\")\n",
        "\n",
        "# Include 'Filename' at the start\n",
        "interleaved_columns =  interleaved_columns\n",
        "\n",
        "# Rename columns to reflect status\n",
        "status_1_data = status_1_data.rename(columns={col: f\"{col}_status_1\" for col in status_1_data.columns if col != 'Filename'})\n",
        "status_0_data = status_0_data.rename(columns={col: f\"{col}_status_0\" for col in status_0_data.columns if col != 'Filename'})\n",
        "\n",
        "# Combine interleaved data\n",
        "combined_data = pd.concat([status_1_data, status_0_data], axis=1)\n",
        "\n",
        "# Rearrange columns in the interleaved order\n",
        "combined_data = combined_data[interleaved_columns]\n",
        "\n",
        "# Save the interleaved data to a CSV file\n",
        "combined_data.to_csv(output_combined_file, index=False)\n",
        "\n",
        "print(f\"Data has been processed and saved to: {output_combined_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1KHY_W48s3d"
      },
      "outputs": [],
      "source": [
        "with_prep_file = 'extracted_features_with_prep.csv'\n",
        "output_combined_file_raw = 'combined_all_features_interleaved_raw.csv'\n",
        "\n",
        "# Read the data from the CSV file\n",
        "with_prep_df = pd.read_csv(with_prep_file)\n",
        "with_prep_df = with_prep_df.drop( columns=[col for col in with_prep_df.columns if \"Code_Number_6digits\" in col])\n",
        "\n",
        "# Ensure the 'Status' column is numeric\n",
        "with_prep_df['Status'] = pd.to_numeric(with_prep_df['Status'], errors='coerce')\n",
        "\n",
        "# Drop rows with NaN in 'Status' if any exist after conversion\n",
        "with_prep_df = with_prep_df.dropna(subset=['Status'])\n",
        "\n",
        "# Separate data based on `Status`\n",
        "status_1_data = with_prep_df[with_prep_df['Status'] == 1].drop(columns=['Status'])\n",
        "status_0_data = with_prep_df[with_prep_df['Status'] == 0].drop(columns=['Status'])\n",
        "\n",
        "# Interleave all features between `Status 1` and `Status 0`\n",
        "interleaved_columns = []\n",
        "for col in with_prep_df.columns:\n",
        "    if col not in ['Filename', 'Status']:  # Exclude non-feature columns\n",
        "        if col in status_1_data.columns:\n",
        "            interleaved_columns.append(f\"{col}_status_1\")\n",
        "        if col in status_0_data.columns:\n",
        "            interleaved_columns.append(f\"{col}_status_0\")\n",
        "\n",
        "# Include 'Filename' at the start\n",
        "interleaved_columns = ['Filename'] + interleaved_columns\n",
        "\n",
        "# Rename columns to reflect status\n",
        "status_1_data = status_1_data.rename(columns={col: f\"{col}_status_1\" for col in status_1_data.columns if col != 'Filename'})\n",
        "status_0_data = status_0_data.rename(columns={col: f\"{col}_status_0\" for col in status_0_data.columns if col != 'Filename'})\n",
        "\n",
        "# Combine interleaved data\n",
        "combined_data = pd.concat([status_1_data, status_0_data], axis=1)\n",
        "\n",
        "# Rearrange columns in the interleaved order\n",
        "combined_data = combined_data[interleaved_columns]\n",
        "\n",
        "# Save the interleaved data to a CSV file\n",
        "combined_data.to_csv(output_combined_file_raw, index=False)\n",
        "\n",
        "print(f\"Data has been processed and saved to: {output_combined_file_raw}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSWED__o8UAF"
      },
      "source": [
        "Visualization for Normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGkSWyUe8S4M"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "data = pd.read_csv(output_combined_file)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "data_dropped = data.drop(\n",
        "    columns=[col for col in data.columns if 'Filename' in col],\n",
        "    errors='ignore'\n",
        ")\n",
        "data_dropped = data_dropped.drop(\n",
        "    columns=['Code_Number_6digits_status_0', 'Code_Number_6digits_status_1'],\n",
        "    errors='ignore'\n",
        ")\n",
        "data_dropped = data_dropped.replace({np.nan: None})\n",
        "\n",
        "# Helper to produce nicer labels\n",
        "def display_name(col):\n",
        "    if col.endswith('_status_1'):\n",
        "        return f\"{col[:-len('_status_1')]} (pfps positive)\"\n",
        "    if col.endswith('_status_0'):\n",
        "        return f\"{col[:-len('_status_0')]} (pfps negative)\"\n",
        "    return col\n",
        "\n",
        "# Visualize features pair-wise with Box Plot\n",
        "def visualize_features_box_pair(pair):\n",
        "    # map raw column names → display names\n",
        "    label_map = {c: display_name(c) for c in pair}\n",
        "\n",
        "    subset_df = (\n",
        "        data_dropped[pair]\n",
        "        .melt(var_name='Feature', value_name='Value')\n",
        "        .dropna()\n",
        "    )\n",
        "    subset_df['Feature'] = subset_df['Feature'].map(label_map)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.boxplot(data=subset_df, x='Feature', y='Value', palette='Set2')\n",
        "\n",
        "    title = \"Box Plot — \" + \" vs \".join(label_map[c] for c in pair)\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.xlabel(\"Feature\", fontsize=12)\n",
        "    plt.ylabel(\"Normalized Value\", fontsize=12)\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize features pair-wise with Violin Plot\n",
        "def visualize_features_violin_pair(pair):\n",
        "    label_map = {c: display_name(c) for c in pair}\n",
        "\n",
        "    subset_df = (\n",
        "        data_dropped[pair]\n",
        "        .melt(var_name='Feature', value_name='Value')\n",
        "        .dropna()\n",
        "    )\n",
        "    subset_df['Feature'] = subset_df['Feature'].map(label_map)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.violinplot(data=subset_df, x='Feature', y='Value', palette='Set2')\n",
        "\n",
        "    title = \"Violin Plot — \" + \" vs \".join(label_map[c] for c in pair)\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.xlabel(\"Feature\", fontsize=12)\n",
        "    plt.ylabel(\"Normalized Value\", fontsize=12)\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "chunk_size = 2\n",
        "columns = data_dropped.columns\n",
        "\n",
        "# Iterate through each pair separately\n",
        "for i in range(0, len(columns), chunk_size):\n",
        "    pair = list(columns[i:i + chunk_size])\n",
        "\n",
        "    visualize_features_box_pair(pair)\n",
        "    visualize_features_violin_pair(pair)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOasM2h-yxqs"
      },
      "source": [
        "split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACWnxEuhyxXc"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('extracted_features_with_prep.csv')\n",
        "print(\"Shape of extracted_features_with_prep:\", df.shape)\n",
        "\n",
        "# Define features and target (adjust 'Status' to your actual target column if different)\n",
        "X = df.drop(columns=['Status', 'Filename'])\n",
        "y = df['Status']\n",
        "\n",
        "# --- Standard Scaler ---\n",
        "standard_scaler = StandardScaler()\n",
        "X_standard = standard_scaler.fit_transform(X)\n",
        "standard_scaled_df = pd.DataFrame(X_standard, columns=X.columns)\n",
        "standard_scaled_df['Status'] = y.values  # Append target\n",
        "print(\"Shape of standard_scaled_df:\", standard_scaled_df.shape)\n",
        "\n",
        "# --- Robust Scaler ---\n",
        "robust_scaler = RobustScaler()\n",
        "X_robust = robust_scaler.fit_transform(X)\n",
        "robust_scaled_df = pd.DataFrame(X_robust, columns=X.columns)\n",
        "robust_scaled_df['Status'] = y.values\n",
        "print(\"Shape of robust_scaled_df:\", robust_scaled_df.shape)\n",
        "\n",
        "# --- MinMax Scaler ---\n",
        "minmax_scaler = MinMaxScaler()\n",
        "X_minmax = minmax_scaler.fit_transform(X)\n",
        "minmax_scaled_df = pd.DataFrame(X_minmax, columns=X.columns)\n",
        "minmax_scaled_df['Status'] = y.values\n",
        "print(\"Shape of minmax_scaled_df:\", minmax_scaled_df.shape)\n",
        "\n",
        "\n",
        "# Output the DataFrames\n",
        "standard_scaled_df, robust_scaled_df, minmax_scaled_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgyPOsXvjn_6"
      },
      "source": [
        "Norm Test after standardization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eWBJ0CUzifGH"
      },
      "outputs": [],
      "source": [
        "def analyze_features(file_path, group_col=\"Group\", group1=\"PFPS\", group2=\"Non-PFPS\", features=None, output_file=None):\n",
        "\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from scipy.stats import shapiro, ttest_ind, mannwhitneyu\n",
        "    import os\n",
        "\n",
        "    # Load the dataset\n",
        "    data = pd.read_csv(file_path)\n",
        "    print(f\"Analyzing file: {file_path}\")\n",
        "    print(data.head())\n",
        "\n",
        "    if features is None:\n",
        "        features = data.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        if group_col in features:\n",
        "            features.remove(group_col)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for feature in features:\n",
        "\n",
        "        if feature not in data.columns:\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nAnalyzing Feature: {feature}\")\n",
        "\n",
        "        # Compute descriptive statistics for each group\n",
        "        mean_feature = data.groupby(group_col)[feature].mean()\n",
        "        median_feature = data.groupby(group_col)[feature].median()\n",
        "        mode_feature = data.groupby(group_col)[feature].agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
        "\n",
        "        # Extract the values for group1 and group2, dropping NaN values\n",
        "        group1_vals = data[data[group_col] == group1][feature].dropna()\n",
        "        group2_vals = data[data[group_col] == group2][feature].dropna()\n",
        "\n",
        "        # Perform Shapiro-Wilk test on group1 values (to assess normality)\n",
        "        try:\n",
        "            stat_shapiro, p_shapiro = shapiro(group1_vals)\n",
        "        except Exception as e:\n",
        "            print(f\"Shapiro test error for {feature}: {e}\")\n",
        "            p_shapiro = np.nan\n",
        "\n",
        "        # Decide which test to use based on normality of group1\n",
        "        if p_shapiro >= 0.05:\n",
        "            test_used = \"t-Test\"\n",
        "            try:\n",
        "                t_stat, p_test = ttest_ind(group1_vals, group2_vals, equal_var=False)\n",
        "            except Exception as e:\n",
        "                print(f\"t-Test error for {feature}: {e}\")\n",
        "                t_stat, p_test = np.nan, np.nan\n",
        "        else:\n",
        "            test_used = \"Mann-Whitney U Test\"\n",
        "            try:\n",
        "                u_stat, p_test = mannwhitneyu(group1_vals, group2_vals, alternative='two-sided')\n",
        "            except Exception as e:\n",
        "                print(f\"Mann-Whitney test error for {feature}: {e}\")\n",
        "                u_stat, p_test = np.nan, np.nan\n",
        "\n",
        "        # Record the results for this feature\n",
        "        results.append({\n",
        "            \"Feature\": feature,\n",
        "            f\"{group1}_Mean\": mean_feature.get(group1, np.nan),\n",
        "            f\"{group2}_Mean\": mean_feature.get(group2, np.nan),\n",
        "            f\"{group1}_Median\": median_feature.get(group1, np.nan),\n",
        "            f\"{group2}_Median\": median_feature.get(group2, np.nan),\n",
        "            f\"{group1}_Mode\": mode_feature.get(group1, np.nan),\n",
        "            f\"{group2}_Mode\": mode_feature.get(group2, np.nan),\n",
        "            \"Shapiro_PValue\": p_shapiro,\n",
        "            \"Test_Used\": test_used,\n",
        "            \"Test_Statistic\": t_stat if test_used==\"t-Test\" else u_stat,\n",
        "            \"Test_PValue\": p_test\n",
        "        })\n",
        "\n",
        "    # Convert results into a DataFrame and save to CSV\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    if output_file is None:\n",
        "        base = os.path.splitext(os.path.basename(file_path))[0]\n",
        "        output_file = f\"{base}_feature_analysis.csv\"\n",
        "\n",
        "    results_df.to_csv(output_file, index=False)\n",
        "    print(f\"\\nAnalysis complete. Results saved to '{output_file}'.\")\n",
        "    return results_df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Save the normalized datasets to CSV files\n",
        "standard_scaled_df.to_csv(\"Standard_norm.csv\", index=False)\n",
        "robust_scaled_df.to_csv(\"Robust_norm.csv\", index=False)\n",
        "minmax_scaled_df.to_csv(\"MinMax_norm.csv\", index=False)\n",
        "\n",
        "analyze_features(\"Standard_norm.csv\", group_col=\"Status\", group1=0, group2=1)\n",
        "analyze_features(\"Robust_norm.csv\", group_col=\"Status\", group1=0, group2=1)\n",
        "analyze_features(\"MinMax_norm.csv\", group_col=\"Status\", group1=0, group2=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuGfav0Bw50U"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"Standard_norm.csv\")\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIk4Awd4isrJ"
      },
      "source": [
        "Visualize after normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HgiYCziY2IWg"
      },
      "outputs": [],
      "source": [
        "# Define the features to plot against \"Status\"\n",
        "features = [\n",
        "      'Mean', 'Std', 'Variance', 'Skewness', 'Kurtosis',\n",
        "       'Dissimilarity_0', 'Dissimilarity_45', 'Dissimilarity_90',\n",
        "       'Dissimilarity_135', 'Correlation_0', 'Correlation_45',\n",
        "       'Correlation_90', 'Correlation_135', 'Homogeneity_0', 'Homogeneity_45',\n",
        "       'Homogeneity_90', 'Homogeneity_135', 'Energy_0', 'Energy_45',\n",
        "       'Energy_90', 'Energy_135', 'Contrast_0', 'Contrast_45', 'Contrast_90',\n",
        "       'Contrast_135', 'RunLengthNonUniformity', 'ShortRunEmphasis',\n",
        "       'LongRunEmphasis', 'GrayLevelNonUniformity', 'RunPercentage',\n",
        "       'RunEntropy', 'Area', 'Eccentricity', 'Perimeter', 'Centroid_X',\n",
        "       'Centroid_Y', 'Hu0',\t'Hu1',\t'Hu2',\t'Hu3',\t'Hu4',\t'Hu5',\t'Hu6', 'Age',\n",
        "       'Gender', 'Active Playing Years', 'Height', 'Weight',\n",
        "       'Leg Length', 'Q-Angle', 'Active Knee'\n",
        "            ]\n",
        "\n",
        "# Function to plot histograms for scaled datasets\n",
        "def plot_histograms(scaled_df, title):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    scaled_df.hist(bins=30, figsize=(20, 15), edgecolor='black')\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "def plot_boxplots(scaled_df, title):\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    sns.boxplot(data=scaled_df, whis=1.5)  # 'whis' controls whisker length\n",
        "    plt.xticks(rotation=90)  # Rotate feature labels for readability\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "# Function to create scatter plots for all features in a single figure\n",
        "def plot_scatter_grid(scaled_df, title):\n",
        "    num_features = len(features)\n",
        "    num_cols = 5  # Number of columns for the grid\n",
        "    num_rows = (num_features // num_cols) + (num_features % num_cols > 0)  # Calculate rows\n",
        "\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 15))\n",
        "    fig.suptitle(title, fontsize=18)\n",
        "    axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
        "\n",
        "    for i, feature in enumerate(features):\n",
        "        # Use the 'Status' column from the DataFrame directly\n",
        "        sns.scatterplot(ax=axes[i], data=scaled_df, x=feature, y='Status', alpha=0.5)\n",
        "        axes[i].set_title(feature)\n",
        "        axes[i].set_xlabel(\"\")\n",
        "        axes[i].set_ylabel(\"Status\")\n",
        "\n",
        "    # Hide any empty subplots\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to fit title\n",
        "    plt.show()\n",
        "\n",
        "# Visualize distributions after applying different scalers\n",
        "plot_histograms(standard_scaled_df, \"Feature Distributions After Standard Scaling\")\n",
        "plot_histograms(robust_scaled_df, \"Feature Distributions After Robust Scaling\")\n",
        "plot_histograms(minmax_scaled_df, \"Feature Distributions After MinMax Scaling\")\n",
        "\n",
        "# Visualize distributions using boxplots for each scaler\n",
        "plot_boxplots(standard_scaled_df, \"Feature Distributions After Standard Scaling (Boxplot)\")\n",
        "plot_boxplots(robust_scaled_df, \"Feature Distributions After Robust Scaling (Boxplot)\")\n",
        "plot_boxplots(minmax_scaled_df, \"Feature Distributions After MinMax Scaling (Boxplot)\")\n",
        "\n",
        "# Generate scatter plots for each scaling method\n",
        "plot_scatter_grid(standard_scaled_df, \"Standard Scaled Data - Scatter Plots\")\n",
        "plot_scatter_grid(robust_scaled_df, \"Robust Scaled Data - Scatter Plots\")\n",
        "plot_scatter_grid(minmax_scaled_df, \"MinMax Scaled Data - Scatter Plots\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQtekjwMzzrJ"
      },
      "source": [
        "Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_PCZ1Bc71Ah"
      },
      "source": [
        "sources:\n",
        "1) source for feature ranking using random forest feature importance and rfe for features selection: https://www.mdpi.com/2224-2708/12/5/67?utm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YarvwZAW5wg"
      },
      "outputs": [],
      "source": [
        "# Define the allowed features list from stat test\n",
        "allowed_features = [\n",
        "\n",
        "    \"Mean\",\n",
        "    \"Skewness\",\n",
        "    \"Kurtosis\",\n",
        "    \"Dissimilarity_0\",\n",
        "    \"Dissimilarity_45\",\n",
        "    \"Dissimilarity_90\",\n",
        "    \"Dissimilarity_135\",\n",
        "    \"Homogeneity_0\",\n",
        "    \"Homogeneity_45\",\n",
        "    \"Homogeneity_90\",\n",
        "    \"Homogeneity_135\",\n",
        "    \"Energy_0\",\n",
        "    \"Energy_45\",\n",
        "    \"Energy_90\",\n",
        "    \"Energy_135\",\n",
        "    \"Contrast_45\",\n",
        "    \"Contrast_90\",\n",
        "    \"RunLengthNonUniformity\",\n",
        "    \"LongRunEmphasis\",\n",
        "    \"GrayLevelNonUniformity\",\n",
        "    \"RunPercentage\",\n",
        "    \"RunEntropy\",\n",
        "    \"Area\",\n",
        "    \"Eccentricity\",\n",
        "    \"Perimeter\",\n",
        "    \"Hu3\",\n",
        "    \"Hu4\",\n",
        "    \"Hu6\",\n",
        "\n",
        "    \"Gender\",\n",
        "    \"Active Playing Years\",\n",
        "    \"Height\",\n",
        "    \"Q-Angle\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('extracted_features_with_prep.csv')\n",
        "print(\"Shape of extracted_features_with_prep:\", df.shape)\n",
        "df = df[allowed_features + ['Status', 'Filename'] ]\n",
        "\n",
        "# Define features and target and non feature\n",
        "X = df.drop(columns=['Status', 'Filename'])\n",
        "y = df['Status']\n",
        "\n",
        "# Use StratifiedKFold to maintain class balance\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Dictionary to store results from each combination of scaler and feature selection method\n",
        "results = {}\n",
        "\n",
        "# Create scaled datasets\n",
        "scaled_data = {}\n",
        "\n",
        "standard_scaler = StandardScaler()\n",
        "robust_scaler = RobustScaler()\n",
        "minmax_scaler = MinMaxScaler()\n",
        "\n",
        "X_standard = standard_scaler.fit_transform(X)\n",
        "X_robust = robust_scaler.fit_transform(X)\n",
        "X_minmax = minmax_scaler.fit_transform(X)\n",
        "\n",
        "df_standard = pd.DataFrame(X_standard, columns=X.columns)\n",
        "df_standard['Status'] = y.values\n",
        "df_robust = pd.DataFrame(X_robust, columns=X.columns)\n",
        "df_robust['Status'] = y.values\n",
        "df_minmax = pd.DataFrame(X_minmax, columns=X.columns)\n",
        "df_minmax['Status'] = y.values\n",
        "\n",
        "scaled_data[\"Standard\"] = df_standard\n",
        "scaled_data[\"Robust\"] = df_robust\n",
        "scaled_data[\"MinMax\"] = df_minmax\n",
        "\n",
        "# Loop over each scaler\n",
        "for scaler_name, df_scaled in scaled_data.items():\n",
        "    print(f\"\\n===== Processing {scaler_name} Scaled Data =====\")\n",
        "    # Start with the full set of features\n",
        "    X_scaled = df_scaled.drop(columns=['Status'])\n",
        "    y_scaled = df_scaled['Status']\n",
        "\n",
        "\n",
        "    # Allowed Features Dropping Method\n",
        "    X_allowed = X_scaled[[col for col in X_scaled.columns if col in allowed_features]]\n",
        "    print(f\"{scaler_name} features after allowed_features filtering: {list(X_allowed.columns)}\")\n",
        "    allowed_df = X_allowed.copy()\n",
        "    allowed_df['Status'] = y_scaled.values\n",
        "    key_allowed = f\"{scaler_name}_Allowed\"\n",
        "    results[key_allowed] = allowed_df.copy()\n",
        "    print(f\"{scaler_name}_Allowed dataset stored with {allowed_df.shape[1]-1} features (excluding 'Status').\")\n",
        "\n",
        "    # RFECV Feature Selection with Logistic Regression\n",
        "    print(f\"\\n=== {scaler_name} Scaling: RFECV Feature Selection (Logistic Regression) with Undersampling ===\")\n",
        "    pipeline_rfecv_lr = ImbPipeline([\n",
        "        ('undersample', RandomUnderSampler(random_state=42)),\n",
        "        ('feature_selection', RFECV(estimator=LogisticRegression(solver='liblinear', penalty='l1', max_iter=1000),\n",
        "                                    cv=5, scoring='accuracy')),\n",
        "        ('clf', LogisticRegression(solver='liblinear', penalty='l1', max_iter=1000))\n",
        "    ])\n",
        "    param_grid_lr = {\n",
        "        'feature_selection__min_features_to_select': [10, 20],\n",
        "        'clf__C': [0.1, 1, 10, 100]\n",
        "    }\n",
        "    nested_cv_lr = GridSearchCV(pipeline_rfecv_lr, param_grid_lr,\n",
        "                             cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "                             scoring='accuracy', n_jobs=-1)\n",
        "    nested_cv_lr.fit(X_scaled, y_scaled)\n",
        "\n",
        "    best_fs_lr = nested_cv_lr.best_estimator_.named_steps['feature_selection']\n",
        "    selected_features_lr = X_scaled.columns[best_fs_lr.support_]\n",
        "    if len(selected_features_lr) == 0:\n",
        "        print(f\"No features were selected for {scaler_name} using L1 Logistic Regression. Consider adjusting C or using L2 penalty.\")\n",
        "    else:\n",
        "        X_rfecv_lr_selected = X_scaled[selected_features_lr]\n",
        "        rfecv_lr_df = X_rfecv_lr_selected.copy()\n",
        "        rfecv_lr_df['Status'] = y_scaled.values\n",
        "        key_rfecv = f\"{scaler_name}_RFECV_LR\"\n",
        "        results[key_rfecv] = rfecv_lr_df\n",
        "\n",
        "        # Visualization for RFECV l1 Logistic Regression\n",
        "        clf_model_lr = nested_cv_lr.best_estimator_.named_steps['clf']\n",
        "        coefs = clf_model_lr.coef_[0]\n",
        "        coef_df = pd.DataFrame({'Feature': selected_features_lr, 'Coefficient': coefs})\n",
        "        if not coef_df.empty:\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            sns.barplot(data=coef_df.sort_values(by='Coefficient', key=abs, ascending=False),\n",
        "                        x='Coefficient', y='Feature', palette='coolwarm')\n",
        "            plt.title(f\"{scaler_name} Scaling: RFECV (LR) Selected Feature Coefficients\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"No features selected for visualization in {scaler_name} RFECV LR.\")\n",
        "\n",
        "\n",
        "    # === PCA Feature Selection with Full Cumulative‑Loading Debugging ===\n",
        "    print(f\"\\n=== {scaler_name} Scaling: PCA Feature Selection ===\")\n",
        "\n",
        "    # Full PCA to inspect all PC\n",
        "    pca_full = PCA()\n",
        "    pca_full.fit(X_scaled)\n",
        "    explained = pca_full.explained_variance_ratio_\n",
        "    cum_var_full = np.cumsum(explained)\n",
        "\n",
        "    # print each PC’s ratio & cumulative for checking\n",
        "    print(\"PC   VarRatio   CumulativeVar\")\n",
        "    for i, (ev, cv) in enumerate(zip(explained, cum_var_full), start=1):\n",
        "        print(f\"PC{i:02d}  {ev:8.4f}     {cv:8.4f}\")\n",
        "\n",
        "    # Plot full cumulative explained variance\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.plot(np.arange(1, len(cum_var_full)+1), cum_var_full, marker='o')\n",
        "    plt.axhline(0.90, color='gray', linestyle='--', label='90% threshold')\n",
        "    plt.xlabel('Principal Component')\n",
        "    plt.ylabel('Cumulative Explained Variance')\n",
        "    plt.title(f\"{scaler_name} Scaling: PCA Cumulative Explained Variance (All PCs)\")\n",
        "    plt.xticks(np.arange(1, len(cum_var_full)+1))\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # PCA up to 90% to get the number of PCs\n",
        "    pca90 = PCA(n_components=0.90)\n",
        "    pca90.fit(X_scaled)\n",
        "    n_pcs_90 = pca90.n_components_\n",
        "    print(f\"Number of PCs to reach ≥90% variance: {n_pcs_90}\")\n",
        "\n",
        "    # Compute loading “mass” over those top PCs\n",
        "    loadings = pca90.components_                          # shape = (n_pcs_90, n_features)\n",
        "    contribs = np.sum(np.abs(loadings), axis=0)\n",
        "    total_loading = contribs.sum()\n",
        "\n",
        "    # Build a DataFrame of feature → contrib → cum_pct\n",
        "    contrib_df = (\n",
        "        pd.DataFrame({\n",
        "            'feature': X_scaled.columns,\n",
        "            'contrib': contribs\n",
        "        })\n",
        "        .sort_values('contrib', ascending=False)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    contrib_df['cum_pct'] = contrib_df['contrib'].cumsum() / total_loading\n",
        "\n",
        "    # Debug print: all features with their cumulative loading percentages\n",
        "    print(\"\\nFeature                     Contrib    CumPct\")\n",
        "    for _, row in contrib_df.iterrows():\n",
        "        print(f\"{row.feature:25s}  {row.contrib:8.4f}  {row.cum_pct:8.4f}\")\n",
        "\n",
        "    # Select features whose cum_pct ≤ 0.90\n",
        "    selected_features_pca = contrib_df.loc[contrib_df['cum_pct'] <= 0.90, 'feature'].tolist()\n",
        "    print(f\"\\nSelected {len(selected_features_pca)} features covering ≤90% loading mass:\")\n",
        "    print(selected_features_pca)\n",
        "\n",
        "    # Store both the full contrib_df and the reduced dataset\n",
        "    results[f\"{scaler_name}_PCA_Loadings\"] = contrib_df.copy()\n",
        "    X_pca_sel = X_scaled[selected_features_pca]\n",
        "    pca_df = pd.DataFrame(X_pca_sel, columns=selected_features_pca)\n",
        "    pca_df['Status'] = y_scaled.values\n",
        "    results[f\"{scaler_name}_PCA\"] = pca_df\n",
        "\n",
        "    # Plot Loading Masses per Feature\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    ax = sns.barplot(data=contrib_df, x='contrib', y='feature', palette='viridis')\n",
        "    ax.set_xlabel('Loading Mass')\n",
        "    ax.set_ylabel('Feature')\n",
        "    ax.set_title(f\"{scaler_name} Scaling: PCA Loading Mass per Feature\")\n",
        "\n",
        "    \"\"\"\n",
        "    # Annotate each bar with its numeric value\n",
        "    for idx, row in contrib_df.iterrows():\n",
        "        ax.text(\n",
        "            row.contrib + 0.02,\n",
        "            idx,\n",
        "            f\"{row.contrib:.2f}\",\n",
        "            va='center'\n",
        "        )\n",
        "    \"\"\"\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Mutual Information Feature Selection\n",
        "    print(f\"\\n=== {scaler_name} Scaling: Mutual Information Feature Selection ===\")\n",
        "    from sklearn.feature_selection import mutual_info_classif\n",
        "    mi_scores = mutual_info_classif(X_scaled, y_scaled, random_state=42)\n",
        "    mi_df = pd.DataFrame({'Feature': X_scaled.columns, 'MI_Score': mi_scores})\n",
        "    mi_df = mi_df.sort_values(by='MI_Score', ascending=False)\n",
        "\n",
        "    # Select features with MI score above the median score\n",
        "    median_mi = mi_df['MI_Score'].median()\n",
        "    selected_features_mi = mi_df[mi_df['MI_Score'] >= median_mi]['Feature'].tolist()\n",
        "    X_mi_selected = X_scaled[selected_features_mi]\n",
        "    mi_df_final = X_mi_selected.copy()\n",
        "    mi_df_final['Status'] = y_scaled.values\n",
        "    key_mi = f\"{scaler_name}_MutualInfo\"\n",
        "    results[key_mi] = mi_df_final\n",
        "\n",
        "    # Visualization for Mutual Information: Feature Scores\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.barplot(data=mi_df, x='MI_Score', y='Feature', palette='Blues_d')\n",
        "    plt.title(f\"{scaler_name} Scaling: Mutual Information Feature Scores\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Save Full Feature Set Without Any Reduction (uses full scaled data)\n",
        "\n",
        "for scaler_name, df_scaled in scaled_data.items():\n",
        "    key_full = f\"{scaler_name}_Full\"\n",
        "    results[key_full] = df_scaled.copy()\n",
        "    print(f\"{scaler_name}_Full dataset stored with {df_scaled.shape[1]-1} features (excluding 'Status').\")\n",
        "\n",
        "# Save all results using a loop\n",
        "for key, result_df in results.items():\n",
        "    filename = f\"final_{key}.csv\"\n",
        "    result_df.to_csv(filename, index=False)\n",
        "    print(f\"{key} complete. Saved as {filename}\")\n",
        "\n",
        "print(\"\\n=== All feature selection methods on scaled data, including PCA and Mutual Information, completed successfully. ===\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFJS2vXTTS9e"
      },
      "outputs": [],
      "source": [
        "lol = \"final_MinMax_PCA.csv\"\n",
        "lol2 = pd.read_csv(lol)\n",
        "lol2.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lol = pd.read_csv(\"extracted_features_with_prep.csv\")\n",
        "\n",
        "lol.columns"
      ],
      "metadata": {
        "id": "ke5iX4X9FF2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjDUa2qykAsq"
      },
      "source": [
        "Norm test after reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BkuiPjDmh5Sd"
      },
      "outputs": [],
      "source": [
        "final_files = [\n",
        "    \"final_MinMax_Allowed.csv\",\n",
        "    \"final_MinMax_RFECV_LR.csv\",\n",
        "    \"final_MinMax_PCA.csv\",\n",
        "    \"final_MinMax_MutualInfo.csv\",\n",
        "    \"final_MinMax_Full.csv\",\n",
        "\n",
        "    \"final_Standard_Allowed.csv\",\n",
        "    \"final_Standard_RFECV_LR.csv\",\n",
        "    \"final_Standard_PCA.csv\",\n",
        "    \"final_Standard_MutualInfo.csv\",\n",
        "    \"final_Standard_Full.csv\",\n",
        "\n",
        "    \"final_Robust_Allowed.csv\",\n",
        "    \"final_Robust_RFECV_LR.csv\",\n",
        "    \"final_Robust_PCA.csv\",\n",
        "    \"final_Robust_MutualInfo.csv\",\n",
        "    \"final_Robust_Full.csv\"\n",
        "]\n",
        "\n",
        "# Loop over each file and analyze it, if it contains the 'Status' column\n",
        "for file in final_files:\n",
        "    try:\n",
        "        df = pd.read_csv(file)\n",
        "        print(f\"\\n\\033[1;34m=== {file} ===\\033[0m\")\n",
        "        print(f\"Shape: {df.shape}\")\n",
        "        display(df.head())\n",
        "        if \"Status\" not in df.columns:\n",
        "            print(f\"Skipping file {file} because it does not contain 'Status'.\")\n",
        "            continue\n",
        "        # Call the analyze_features function (ensure it is defined or imported).\n",
        "        # Adjust group1 and group2 values as needed.\n",
        "        analyze_features(file, group_col=\"Status\", group1=0, group2=1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "# Download all final output CSV files.\n",
        "analysis_files = glob.glob(\"final_*_feature_analysis.csv\") + glob.glob(\"final_*_feature_ranking.csv\")\n",
        "all_files = final_files + analysis_files\n",
        "\n",
        "for file in all_files:\n",
        "    if os.path.isfile(file):\n",
        "        print(\"Downloading:\", file)\n",
        "        files.download(file)\n",
        "    else:\n",
        "        print(f\"File not found: {file}\")\n",
        "\n",
        "print(\"\\n=== Download of all final output files completed. ===\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kXQPRBp7CGM"
      },
      "source": [
        "Validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR1g73GlTb6H"
      },
      "outputs": [],
      "source": [
        "lol = \"final_Standard_Allowed.csv\"\n",
        "lol2 = pd.read_csv(lol)\n",
        "lol2.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPiunc79lq1v"
      },
      "source": [
        "with MSE, with Grid Search F1 score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3AWcqkTlqjP"
      },
      "outputs": [],
      "source": [
        "# Suppress warnings for undefined metrics and deprecated messages.\n",
        "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\"`use_label_encoder` is deprecated\")\n",
        "\n",
        "\n",
        "# Learning Curve Function using Training Loss (Log Loss)\n",
        "def plot_learning_curve(estimator, title, X, y, cv, scoring):\n",
        "    train_sizes, train_scores, val_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, scoring=scoring, n_jobs=-1\n",
        "    )\n",
        "\n",
        "    if scoring == 'neg_log_loss':\n",
        "        train_loss = -np.mean(train_scores, axis=1)\n",
        "        val_loss = -np.mean(val_scores, axis=1)\n",
        "    else:\n",
        "        train_loss = 1 - np.mean(train_scores, axis=1)\n",
        "        val_loss = 1 - np.mean(val_scores, axis=1)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(train_sizes, train_loss, 'o-', color=\"r\", label=\"Training Loss\")\n",
        "    plt.plot(train_sizes, val_loss, 'o-', color=\"g\", label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Number of Training Samples\")\n",
        "    plt.ylabel(\"Log Loss\")\n",
        "    plt.title(title)\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Updated Function for Training vs. Testing Error using Log Loss\n",
        "def plot_train_test_error(model, X_train, y_train, X_test, y_test, model_name, method_name, scoring=\"neg_log_loss\"):\n",
        "\n",
        "    try:\n",
        "        y_train_pred_prob = model.predict_proba(X_train)\n",
        "        y_test_pred_prob = model.predict_proba(X_test)\n",
        "    except Exception as e:\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        y_test_pred = model.predict(X_test)\n",
        "        y_train_pred_prob = np.vstack([1 - y_train_pred, y_train_pred]).T\n",
        "        y_test_pred_prob = np.vstack([1 - y_test_pred, y_test_pred]).T\n",
        "\n",
        "    train_loss = log_loss(y_train, y_train_pred_prob)\n",
        "    test_loss = log_loss(y_test, y_test_pred_prob)\n",
        "    metric_label = \"Log Loss\"\n",
        "\n",
        "    loss_difference = test_loss - train_loss\n",
        "    loss_ratio = test_loss / train_loss if train_loss != 0 else float('inf')\n",
        "\n",
        "    print(f\"{model_name} ({method_name}):\")\n",
        "    print(f\"  Training Loss: {train_loss:.4f}\")\n",
        "    print(f\"  Testing Loss:  {test_loss:.4f}\")\n",
        "    print(f\"  Loss Difference (Test - Train): {loss_difference:.4f}\")\n",
        "    print(f\"  Loss Ratio (Test / Train): {loss_ratio:.2f}\")\n",
        "\n",
        "    if loss_ratio > 1.2:\n",
        "        print(\"  Warning: Test Loss is significantly higher than Training Loss. Possible overfitting detected.\")\n",
        "    else:\n",
        "        print(\"  Training and Testing Loss are similar. Overfitting is less likely.\")\n",
        "\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.bar([\"Train Loss\", \"Test Loss\"], [train_loss, test_loss], color=[\"#1f77b4\", \"#ff7f0e\"])\n",
        "    plt.title(f\"Training vs. Testing Loss\\n{model_name} ({method_name})\")\n",
        "    plt.ylabel(metric_label)\n",
        "    plt.grid(axis='y', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Precision-Recall Plotting Function\n",
        "def plot_precision_recall_and_threshold(y_true, y_scores, model_name, method_name):\n",
        "    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
        "    # Compute F1 scores for thresholds (thresholds has one less element)\n",
        "    f1_scores = 2 * precision[:-1] * recall[:-1] / (precision[:-1] + recall[:-1] + 1e-10)\n",
        "    best_idx = np.argmax(f1_scores)\n",
        "    best_threshold = thresholds[best_idx]\n",
        "    print(f\"Optimal threshold for {model_name} ({method_name}) based on max F1 score: {best_threshold:.2f}\")\n",
        "    avg_precision = average_precision_score(y_true, y_scores)\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.plot(recall, precision, label=f'AP = {avg_precision:.2f}')\n",
        "    plt.scatter(recall[best_idx], precision[best_idx], color='red',\n",
        "                label=f'Optimal (F1 = {f1_scores[best_idx]:.2f})')\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.title(f\"Precision-Recall Curve: {model_name} ({method_name})\\n(Maximizing F1 Score)\")\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return best_threshold\n",
        "\n",
        "\n",
        "# Load Feature Selection Outputs\n",
        "print(\"=== Step 1: Load Feature Selection Outputs ===\")\n",
        "scalers = [\"Standard\", \"MinMax\", \"Robust\"]\n",
        "methods = [\"Allowed\", \"RFECV_LR\", \"PCA\", \"MutualInfo\", \"Full\"]\n",
        "\n",
        "feature_selection_methods = {}\n",
        "for scaler in scalers:\n",
        "    for method in methods:\n",
        "        key = f\"{scaler}_{method}\"\n",
        "        filename = f\"final_{key}.csv\"\n",
        "        try:\n",
        "            df = pd.read_csv(filename)\n",
        "            feature_selection_methods[key] = df\n",
        "            print(f\"Loaded {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {filename}: {e}\")\n",
        "\n",
        "\n",
        "# Define Models for Testing\n",
        "def define_models():\n",
        "    return {\n",
        "        \"SVM\": SVC(kernel='linear', probability=True, random_state=42, max_iter=5000),\n",
        "        \"SVM_RBF\": SVC(kernel='rbf', probability=True, random_state=42, max_iter=5000),   # Added SVM RBF\n",
        "        \"SVM_Poly\": SVC(kernel='poly', probability=True, random_state=42, max_iter=5000),  # Added SVM Pol\n",
        "        \"L1LogisticRegression\": LogisticRegression(penalty='l1', solver='liblinear',\n",
        "                                                   class_weight='balanced', random_state=42, max_iter=1000),\n",
        "        \"LDA\": LinearDiscriminantAnalysis()\n",
        "    }\n",
        "\n",
        "# Set the grid search scoring metric to maximize\n",
        "grid_search_scoring = \"f1\"\n",
        "\n",
        "\n",
        "# Define function to train and evaluate models with Partial Undersampling + Class Weighting and visualizations\n",
        "def train_and_evaluate_models(X_df, method_name):\n",
        "    print(f\"\\n\\033[1;34m=== TESTING FEATURE SELECTION METHOD: {method_name} ===\\033[0m\")\n",
        "\n",
        "    # Partial Undersampling Before Splitting with Class Weighting\n",
        "    df_combined = X_df.copy()\n",
        "    counts = df_combined['Status'].value_counts()\n",
        "    if len(counts) == 2:\n",
        "        minority_class = counts.idxmin()\n",
        "        majority_class = counts.idxmax()\n",
        "        minority_count = counts.min()\n",
        "        desired_ratio = 2\n",
        "        df_minority = df_combined[df_combined['Status'] == minority_class]\n",
        "        df_majority = df_combined[df_combined['Status'] == majority_class]\n",
        "        sample_size = min(len(df_majority), desired_ratio * minority_count)\n",
        "        df_majority_sampled = df_majority.sample(sample_size, random_state=42)\n",
        "        df_undersampled = pd.concat([df_minority, df_majority_sampled])\n",
        "    else:\n",
        "        df_undersampled = df_combined\n",
        "    print(\"Data shape after partial undersampling:\", df_undersampled.shape)\n",
        "\n",
        "    # Separate features and target\n",
        "    X_features = df_undersampled.drop(columns=['Status'])\n",
        "    y_target = df_undersampled['Status']\n",
        "\n",
        "    # Train-Test Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_features, y_target, test_size=0.2, random_state=42, stratify=y_target\n",
        "    )\n",
        "    print(\"Training set shape:\", X_train.shape)\n",
        "    print(\"Testing set shape:\", X_test.shape)\n",
        "\n",
        "    models = define_models()\n",
        "    results_local = {}\n",
        "    optimized_local = {}\n",
        "\n",
        "    def plot_confusion_matrix(cm, title):\n",
        "        plt.figure(figsize=(6,4))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "        plt.title(title)\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def get_scoring(estimator):\n",
        "\n",
        "        return 'neg_log_loss'\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\n=== Training and Evaluating Model: {name} ({method_name}) ===\")\n",
        "        scoring_metric = get_scoring(model)\n",
        "\n",
        "        # Learning Curve for the Initial (Pre-Optimized) Model\n",
        "        print(f\"\\n--- Learning Curve for initial {name} model ({method_name}) ---\")\n",
        "        plot_learning_curve(\n",
        "            model,\n",
        "            f\"Initial {name} Learning Curve ({method_name})\",\n",
        "            X_train, y_train,\n",
        "            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "            scoring=scoring_metric\n",
        "        )\n",
        "\n",
        "        # Pre-Optimized Train vs. Test Error (Using Log Loss)\n",
        "        print(f\"\\n--- Pre-Optimized {name} Train vs. Test Error ({method_name}) ---\")\n",
        "        try:\n",
        "            model.fit(X_train, y_train)\n",
        "            plot_train_test_error(\n",
        "                model, X_train, y_train, X_test, y_test,\n",
        "                model_name=f\"{name}_PreOptim\", method_name=method_name,\n",
        "                scoring=scoring_metric\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error fitting pre-optimized {name}: {e}\")\n",
        "\n",
        "        # PRE-OPTIMIZED KFold Cross-Validation Accuracy\n",
        "        print(f\"\\n--- Pre-Optimized KFold CV Accuracy for {name} ({method_name}) ---\")\n",
        "        cv_pre = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        pre_fold_accuracies = []\n",
        "        for fold, (train_idx, val_idx) in enumerate(cv_pre.split(X_train, y_train), 1):\n",
        "            X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "            y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "            model.fit(X_fold_train, y_fold_train)\n",
        "            acc = model.score(X_fold_val, y_fold_val)\n",
        "            pre_fold_accuracies.append(acc)\n",
        "            print(f\"{name} (Pre-Optimized) - Fold {fold} Accuracy: {acc:.4f}\")\n",
        "        mean_pre_acc = np.mean(pre_fold_accuracies)\n",
        "        print(f\"{name} (Pre-Optimized) - Mean CV Accuracy: {mean_pre_acc:.4f}\")\n",
        "        plt.figure(figsize=(6,4))\n",
        "        plt.bar(range(1, len(pre_fold_accuracies)+1), pre_fold_accuracies, color=\"#2ca02c\")\n",
        "        plt.title(f\"Pre-Optimized CV Accuracy per Fold for {name}\")\n",
        "        plt.xlabel(\"Fold Number\")\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        plt.ylim(0, 1)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Hyperparameter Tuning / Grid Search using f1 score\n",
        "        try:\n",
        "            if name == \"L1LogisticRegression\":\n",
        "                param_grid = {\"clf__class_weight\": [{0: 1, 1: w} for w in [2, 3, 4, 5, 6]]}\n",
        "                pipeline = Pipeline([('clf', model)])\n",
        "                grid = GridSearchCV(\n",
        "                    pipeline, param_grid, scoring=grid_search_scoring,\n",
        "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), n_jobs=-1\n",
        "                )\n",
        "                grid.fit(X_train, y_train)\n",
        "                print(f\"Best parameters for {name}: {grid.best_params_}\")\n",
        "                model_optimized = grid.best_estimator_.named_steps['clf']\n",
        "\n",
        "            elif name in [\"SVM\", \"SVM_RBF\"]:\n",
        "                param_grid = {\"clf__C\": [0.01, 0.1, 1, 10, 100]}\n",
        "                if name == \"SVM_RBF\":\n",
        "                    param_grid[\"clf__gamma\"] = ['scale', 'auto']\n",
        "                pipeline = Pipeline([('clf', model)])\n",
        "                grid = GridSearchCV(\n",
        "                    pipeline, param_grid, scoring=grid_search_scoring,\n",
        "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), n_jobs=-1\n",
        "                )\n",
        "                grid.fit(X_train, y_train)\n",
        "                print(f\"Best parameters for {name}: {grid.best_params_}\")\n",
        "                model_optimized = grid.best_estimator_.named_steps['clf']\n",
        "\n",
        "            elif name == \"SVM_Poly\":\n",
        "                param_grid = {\"clf__C\": [0.01, 0.1, 1, 10, 100],\n",
        "                              \"clf__degree\": [2, 3, 4]}\n",
        "                pipeline = Pipeline([('clf', model)])\n",
        "                grid = GridSearchCV(\n",
        "                    pipeline, param_grid, scoring=grid_search_scoring,\n",
        "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), n_jobs=-1\n",
        "                )\n",
        "                grid.fit(X_train, y_train)\n",
        "                print(f\"Best parameters for {name}: {grid.best_params_}\")\n",
        "                model_optimized = grid.best_estimator_.named_steps['clf']\n",
        "\n",
        "            elif name == \"LDA\":\n",
        "                param_grid = {\n",
        "                    \"clf__solver\": [\"lsqr\", \"eigen\"],\n",
        "                    \"clf__shrinkage\": [0.3, 0.5, 0.7]\n",
        "                }\n",
        "                pipeline = Pipeline([('clf', model)])\n",
        "                grid = GridSearchCV(\n",
        "                    pipeline, param_grid, scoring=grid_search_scoring,\n",
        "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "                    n_jobs=-1, error_score='raise'\n",
        "                )\n",
        "                grid.fit(X_train, y_train)\n",
        "                print(f\"Best parameters for {name}: {grid.best_params_}\")\n",
        "                model_optimized = grid.best_estimator_.named_steps['clf']\n",
        "\n",
        "            else:\n",
        "                model_optimized = model\n",
        "\n",
        "            optimized_local[name] = model_optimized\n",
        "\n",
        "            # Learning Curve for the Optimized Model\n",
        "            print(f\"\\n--- Learning Curve for optimized {name} model ({method_name}) ---\")\n",
        "            plot_learning_curve(\n",
        "                model_optimized,\n",
        "                f\"Optimized {name} Learning Curve ({method_name})\",\n",
        "                X_train, y_train,\n",
        "                cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "                scoring=scoring_metric\n",
        "            )\n",
        "\n",
        "            # Post-Optimized Train vs. Test Error (Using Log Loss)\n",
        "            print(f\"\\n--- Post-Optimized {name} Train vs. Test Loss ({method_name}) ---\")\n",
        "            model_optimized.fit(X_train, y_train)\n",
        "            plot_train_test_error(\n",
        "                model_optimized, X_train, y_train, X_test, y_test,\n",
        "                model_name=f\"{name}_PostOptim\", method_name=method_name,\n",
        "                scoring=scoring_metric\n",
        "            )\n",
        "\n",
        "            # Post-Optimized KFold Cross-Validation Accuracy Visualization\n",
        "            cv_post = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "            post_fold_accuracies = []\n",
        "            for fold, (train_idx, val_idx) in enumerate(cv_post.split(X_train, y_train), 1):\n",
        "                X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "                y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "                model_optimized.fit(X_fold_train, y_fold_train)\n",
        "                acc = model_optimized.score(X_fold_val, y_fold_val)\n",
        "                post_fold_accuracies.append(acc)\n",
        "                print(f\"{name} (Post-Optimized) - Fold {fold} Accuracy: {acc:.4f}\")\n",
        "            mean_post_acc = np.mean(post_fold_accuracies)\n",
        "            print(f\"{name} (Post-Optimized) - Mean CV Accuracy: {mean_post_acc:.4f}\")\n",
        "            plt.figure(figsize=(6,4))\n",
        "            plt.bar(range(1, len(post_fold_accuracies)+1), post_fold_accuracies, color=\"#2ca02c\")\n",
        "            plt.title(f\"Post-Optimized CV Accuracy per Fold for {name}\")\n",
        "            plt.xlabel(\"Fold Number\")\n",
        "            plt.ylabel(\"Accuracy\")\n",
        "            plt.ylim(0, 1)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Unadjusted and Adjusted Confusion Matrices\n",
        "            if hasattr(model_optimized, \"predict_proba\"):\n",
        "                # Compute and display unadjusted predictions and confusion matrix.\n",
        "                y_pred = model_optimized.predict(X_test)\n",
        "                cm_unadjusted = confusion_matrix(y_test, y_pred)\n",
        "                report_unadjusted = classification_report(y_test, y_pred, zero_division=0, output_dict=True)\n",
        "                print(f\"\\nUnadjusted Classification Report for {name} ({method_name}):\")\n",
        "                print(classification_report(y_test, y_pred, zero_division=0))\n",
        "                plot_confusion_matrix(cm_unadjusted, f\"Unadjusted Confusion Matrix: {name} ({method_name})\")\n",
        "\n",
        "                # Now apply threshold adjustment using precision-recall.\n",
        "                y_scores = model_optimized.predict_proba(X_test)[:, 1]\n",
        "                best_threshold = plot_precision_recall_and_threshold(\n",
        "                    y_test, y_scores, model_name=name, method_name=method_name\n",
        "                )\n",
        "                y_pred_thresh = (y_scores >= best_threshold).astype(int)\n",
        "                report_adjusted = classification_report(y_test, y_pred_thresh, zero_division=0, output_dict=True)\n",
        "                print(f\"\\nAdjusted Classification Report at threshold {best_threshold:.2f} for {name} ({method_name}):\")\n",
        "                print(classification_report(y_test, y_pred_thresh, zero_division=0))\n",
        "                cm_adjusted = confusion_matrix(y_test, y_pred_thresh)\n",
        "                plot_confusion_matrix(cm_adjusted, f\"Final Adjusted Confusion Matrix: {name} ({method_name})\")\n",
        "                results_local[name] = {\n",
        "                    \"confusion_matrix_unadjusted\": cm_unadjusted,\n",
        "                    \"classification_report_unadjusted\": report_unadjusted,\n",
        "                    \"confusion_matrix_adjusted\": cm_adjusted,\n",
        "                    \"classification_report_adjusted\": report_adjusted,\n",
        "                    \"FeatureSelectionMethod\": method_name\n",
        "                }\n",
        "            else:\n",
        "                y_pred = model_optimized.predict(X_test)\n",
        "                cm = confusion_matrix(y_test, y_pred)\n",
        "                report = classification_report(y_test, y_pred, zero_division=0, output_dict=True)\n",
        "                plot_confusion_matrix(cm, f\"Final Confusion Matrix: {name} ({method_name})\")\n",
        "                results_local[name] = {\n",
        "                    \"confusion_matrix\": cm,\n",
        "                    \"classification_report_unadjusted\": report,\n",
        "                    \"FeatureSelectionMethod\": method_name\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating {name} on {method_name}: {e}\")\n",
        "            results_local[name] = {\"error\": str(e), \"FeatureSelectionMethod\": method_name}\n",
        "\n",
        "    return {\"performance\": results_local, \"optimized_models\": optimized_local}\n",
        "\n",
        "\n",
        "# Run Evaluations on Each Feature Selection Method\n",
        "evaluation_results = {}\n",
        "for method_key, df in feature_selection_methods.items():\n",
        "    evaluation_results[method_key] = train_and_evaluate_models(df, method_key)\n",
        "\n",
        "print(\"\\n=== Model Evaluation Completed for All Feature Selection Methods ===\")\n",
        "\n",
        "# Save evaluation results\n",
        "joblib.dump(evaluation_results, 'evaluation_results.pkl')\n",
        "print(\"Evaluation results saved to 'evaluation_results.pkl'\")\n",
        "\n",
        "optimized_models = {}\n",
        "for method_key, result in evaluation_results.items():\n",
        "    optimized_models[method_key] = result.get(\"optimized_models\", {})\n",
        "joblib.dump(optimized_models, 'optimized_models.pkl')\n",
        "print(\"Optimized models saved to 'optimized_models.pkl'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR-DXgVaThqW"
      },
      "outputs": [],
      "source": [
        "lol = \"final_Standard_RFECV_LR.csv\"\n",
        "lol2 = pd.read_csv(lol)\n",
        "lol2.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v-1MsmHow-x"
      },
      "source": [
        "Grid search multiply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DximsVEQowzp"
      },
      "outputs": [],
      "source": [
        "# Suppress warnings for undefined metrics and deprecated messages.\n",
        "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\"`use_label_encoder` is deprecated\")\n",
        "\n",
        "\n",
        "# Product of Precision & Recall\n",
        "def precision_recall_product(y_true, y_pred):\n",
        "    p = precision_score(y_true, y_pred, zero_division=0)\n",
        "    r = recall_score(y_true, y_pred, zero_division=0)\n",
        "    return p * r\n",
        "\n",
        "product_pr_scorer = make_scorer(precision_recall_product, greater_is_better=True)\n",
        "\n",
        "# Learning Curve Function using Training Loss (Log Loss)\n",
        "def plot_learning_curve(estimator, title, X, y, cv, scoring):\n",
        "    \"\"\"\n",
        "    Plots the learning curve and returns training sizes, training loss, and validation loss.\n",
        "    \"\"\"\n",
        "    train_sizes, train_scores, val_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, scoring=scoring, n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Convert negative log loss to positive loss if needed.\n",
        "    if scoring == 'neg_log_loss':\n",
        "        train_loss = -np.mean(train_scores, axis=1)\n",
        "        val_loss = -np.mean(val_scores, axis=1)\n",
        "    else:\n",
        "        train_loss = 1 - np.mean(train_scores, axis=1)\n",
        "        val_loss = 1 - np.mean(val_scores, axis=1)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(train_sizes, train_loss, 'o-', color=\"r\", label=\"Training Loss\")\n",
        "    plt.plot(train_sizes, val_loss, 'o-', color=\"g\", label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Number of Training Samples\")\n",
        "    plt.ylabel(\"Log Loss\")\n",
        "    plt.title(title)\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return train_sizes, train_loss, val_loss\n",
        "\n",
        "# flatten nested dictionaries for CSV storage\n",
        "def flatten_dict(d, parent_key='', sep='_'):\n",
        "    items = []\n",
        "    for k, v in d.items():\n",
        "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
        "        if isinstance(v, dict):\n",
        "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
        "        else:\n",
        "            items.append((new_key, v))\n",
        "    return dict(items)\n",
        "\n",
        "\n",
        "# Updated Train-vs-Test Error Function Using Log Loss\n",
        "def plot_train_test_error(model, X_train, y_train, X_test, y_test, model_name, method_name, scoring=\"neg_log_loss\"):\n",
        "    \"\"\"\n",
        "    Plots a bar chart comparing training and testing log loss,\n",
        "    and prints the loss difference and ratio.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        y_train_pred_prob = model.predict_proba(X_train)\n",
        "        y_test_pred_prob = model.predict_proba(X_test)\n",
        "    except Exception as e:\n",
        "        print(\"Model does not support predict_proba, falling back to dummy conversion.\")\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        y_test_pred = model.predict(X_test)\n",
        "        y_train_pred_prob = np.vstack([1 - y_train_pred, y_train_pred]).T\n",
        "        y_test_pred_prob = np.vstack([1 - y_test_pred, y_test_pred]).T\n",
        "\n",
        "    train_loss = log_loss(y_train, y_train_pred_prob)\n",
        "    test_loss = log_loss(y_test, y_test_pred_prob)\n",
        "    metric_label = \"Log Loss\"\n",
        "\n",
        "    loss_difference = test_loss - train_loss\n",
        "    loss_ratio = test_loss / train_loss if train_loss != 0 else float('inf')\n",
        "\n",
        "    print(f\"{model_name} ({method_name}):\")\n",
        "    print(f\"  Training Loss: {train_loss:.4f}\")\n",
        "    print(f\"  Testing Loss:  {test_loss:.4f}\")\n",
        "    print(f\"  Loss Difference (Test - Train): {loss_difference:.4f}\")\n",
        "    print(f\"  Loss Ratio (Test / Train): {loss_ratio:.2f}\")\n",
        "\n",
        "    if loss_ratio > 1.2:\n",
        "        print(\"  Warning: Test Loss is significantly higher than Training Loss. Possible overfitting detected.\")\n",
        "    else:\n",
        "        print(\"  Training and Testing Loss are similar. Overfitting is less likely.\")\n",
        "\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.bar([\"Train Loss\", \"Test Loss\"], [train_loss, test_loss], color=[\"#1f77b4\", \"#ff7f0e\"])\n",
        "    plt.title(f\"Training vs. Testing Loss\\n{model_name} ({method_name})\")\n",
        "    plt.ylabel(metric_label)\n",
        "    plt.grid(axis='y', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Precision-Recall Plotting Function Using Product Metric\n",
        "def plot_precision_recall_and_threshold(y_true, y_scores, model_name, method_name):\n",
        "    \"\"\"\n",
        "    Plots the Precision-Recall curve and finds the threshold where the product\n",
        "    of precision and recall is maximized.\n",
        "    \"\"\"\n",
        "    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
        "    # Calculate product of precision and recall for each threshold (exclude last point)\n",
        "    prod_scores = precision[:-1] * recall[:-1]\n",
        "    best_idx = np.argmax(prod_scores)\n",
        "    best_threshold = thresholds[best_idx]\n",
        "    print(f\"Optimal threshold for {model_name} ({method_name}) based on max (Precision x Recall): {best_threshold:.2f}\")\n",
        "    avg_precision = average_precision_score(y_true, y_scores)\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.plot(recall, precision, label=f'AP = {avg_precision:.2f}')\n",
        "    plt.scatter(recall[best_idx], precision[best_idx], color='red',\n",
        "                label=f'Optimal (Prod = {prod_scores[best_idx]:.2f})')\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.title(f\"Precision-Recall Curve: {model_name} ({method_name})\\n(Maximizing Product)\")\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return best_threshold\n",
        "\n",
        "\n",
        "# Load Feature Selection Outputs\n",
        "print(\"=== Step 1: Load Feature Selection Outputs ===\")\n",
        "\n",
        "scalers = [\"Standard\", \"MinMax\", \"Robust\"]\n",
        "methods = [\"Allowed\", \"RFECV_LR\", \"PCA\", \"MutualInfo\", \"Full\"]\n",
        "\n",
        "feature_selection_methods = {}\n",
        "for scaler in scalers:\n",
        "    for method in methods:\n",
        "        key = f\"{scaler}_{method}\"\n",
        "        filename = f\"final_{key}.csv\"\n",
        "        try:\n",
        "            df = pd.read_csv(filename)\n",
        "            feature_selection_methods[key] = df\n",
        "            print(f\"Loaded {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {filename}: {e}\")\n",
        "\n",
        "\n",
        "# === Define Models for Testing ===\n",
        "def define_models():\n",
        "    return {\n",
        "        \"SVM\": SVC(kernel='linear', probability=True, random_state=42, max_iter=5000),\n",
        "        \"SVM_RBF\": SVC(kernel='rbf', probability=True, random_state=42, max_iter=5000),\n",
        "        \"SVM_Poly\": SVC(kernel='poly', probability=True, random_state=42, max_iter=5000),\n",
        "        \"L1LogisticRegression\": LogisticRegression(penalty='l1', solver='liblinear',\n",
        "                                                   class_weight='balanced', random_state=42, max_iter=1000),\n",
        "        \"LDA\": LinearDiscriminantAnalysis()\n",
        "    }\n",
        "\n",
        "# Set the grid search scoring metric to maximize (traditionally f1 for classification)\n",
        "grid_search_scoring = \"f1\"\n",
        "\n",
        "\n",
        "# Train & Evaluate Models\n",
        "def train_and_evaluate_models(X_df, method_name):\n",
        "    print(f\"\\n\\033[1;34m=== TESTING FEATURE SELECTION METHOD: {method_name} ===\\033[0m\")\n",
        "\n",
        "    # Partial Undersampling\n",
        "    df_combined = X_df.copy()\n",
        "    counts = df_combined['Status'].value_counts()\n",
        "    if len(counts) == 2:\n",
        "        minority_class = counts.idxmin()\n",
        "        majority_class = counts.idxmax()\n",
        "        minority_count = counts.min()\n",
        "        desired_ratio = 2\n",
        "        df_minority = df_combined[df_combined['Status'] == minority_class]\n",
        "        df_majority = df_combined[df_combined['Status'] == majority_class]\n",
        "        sample_size = min(len(df_majority), desired_ratio * minority_count)\n",
        "        df_majority_sampled = df_majority.sample(sample_size, random_state=42)\n",
        "        df_undersampled = pd.concat([df_minority, df_majority_sampled])\n",
        "    else:\n",
        "        df_undersampled = df_combined\n",
        "    print(\"Data shape after partial undersampling:\", df_undersampled.shape)\n",
        "\n",
        "    # Separate features and target\n",
        "    X_features = df_undersampled.drop(columns=['Status'])\n",
        "    y_target = df_undersampled['Status']\n",
        "\n",
        "    # Train-Test Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_features, y_target, test_size=0.2, random_state=42, stratify=y_target\n",
        "    )\n",
        "    print(\"Training set shape:\", X_train.shape)\n",
        "    print(\"Testing set shape:\", X_test.shape)\n",
        "\n",
        "    models = define_models()\n",
        "    results_local = {}\n",
        "    optimized_local = {}\n",
        "\n",
        "    def plot_confusion_matrix(cm, title):\n",
        "        plt.figure(figsize=(6,4))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "        plt.title(title)\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def get_scoring(estimator):\n",
        "        return 'neg_log_loss'\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\n=== Training and Evaluating Model: {name} ({method_name}) ===\")\n",
        "        scoring_metric = get_scoring(model)\n",
        "\n",
        "        # Learning Curve (Pre-Optimized)\n",
        "        print(f\"\\n--- Learning Curve for initial {name} model ({method_name}) ---\")\n",
        "        plot_learning_curve(\n",
        "            model,\n",
        "            f\"Initial {name} Learning Curve ({method_name})\",\n",
        "            X_train, y_train,\n",
        "            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "            scoring=scoring_metric\n",
        "        )\n",
        "\n",
        "        # Pre-Optimized Train vs. Test Error (Using Log Loss)\n",
        "        print(f\"\\n--- Pre-Optimized {name} Train vs. Test Error ({method_name}) ---\")\n",
        "        try:\n",
        "            model.fit(X_train, y_train)\n",
        "            plot_train_test_error(\n",
        "                model, X_train, y_train, X_test, y_test,\n",
        "                model_name=f\"{name}_PreOptim\", method_name=method_name,\n",
        "                scoring=scoring_metric\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error fitting pre-optimized {name}: {e}\")\n",
        "\n",
        "        # Pre-Optimized KFold CV Accuracy\n",
        "        print(f\"\\n--- Pre-Optimized KFold CV Accuracy for {name} ({method_name}) ---\")\n",
        "        cv_pre = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        pre_fold_accuracies = []\n",
        "        for fold, (train_idx, val_idx) in enumerate(cv_pre.split(X_train, y_train), 1):\n",
        "            X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "            y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "            model.fit(X_fold_train, y_fold_train)\n",
        "            acc = model.score(X_fold_val, y_fold_val)\n",
        "            pre_fold_accuracies.append(acc)\n",
        "            print(f\"{name} (Pre-Optimized) - Fold {fold} Accuracy: {acc:.4f}\")\n",
        "        mean_pre_acc = np.mean(pre_fold_accuracies)\n",
        "        print(f\"{name} (Pre-Optimized) - Mean CV Accuracy: {mean_pre_acc:.4f}\")\n",
        "        plt.figure(figsize=(6,4))\n",
        "        plt.bar(range(1, len(pre_fold_accuracies)+1), pre_fold_accuracies, color=\"#2ca02c\")\n",
        "        plt.title(f\"Pre-Optimized CV Accuracy per Fold for {name}\")\n",
        "        plt.xlabel(\"Fold Number\")\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        plt.ylim(0, 1)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Grid Search using Product of Precision & Recall\n",
        "        try:\n",
        "            if name == \"L1LogisticRegression\":\n",
        "                param_grid = {\"clf__class_weight\": [{0: 1, 1: w} for w in [2, 3, 4, 5, 6]]}\n",
        "                pipeline = Pipeline([('clf', model)])\n",
        "                grid = GridSearchCV(\n",
        "                    pipeline, param_grid, scoring=product_pr_scorer,\n",
        "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), n_jobs=-1\n",
        "                )\n",
        "                grid.fit(X_train, y_train)\n",
        "                print(f\"Best parameters for {name}: {grid.best_params_}\")\n",
        "                model_optimized = grid.best_estimator_.named_steps['clf']\n",
        "\n",
        "            elif name in [\"SVM\", \"SVM_RBF\"]:\n",
        "                param_grid = {\"clf__C\": [0.01, 0.1, 1, 10, 100]}\n",
        "                if name == \"SVM_RBF\":\n",
        "                    param_grid[\"clf__gamma\"] = ['scale', 'auto']\n",
        "                pipeline = Pipeline([('clf', model)])\n",
        "                grid = GridSearchCV(\n",
        "                    pipeline, param_grid, scoring=product_pr_scorer,\n",
        "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), n_jobs=-1\n",
        "                )\n",
        "                grid.fit(X_train, y_train)\n",
        "                print(f\"Best parameters for {name}: {grid.best_params_}\")\n",
        "                model_optimized = grid.best_estimator_.named_steps['clf']\n",
        "\n",
        "            elif name == \"SVM_Poly\":\n",
        "                param_grid = {\"clf__C\": [0.01, 0.1, 1, 10, 100],\n",
        "                              \"clf__degree\": [2, 3, 4]}\n",
        "                pipeline = Pipeline([('clf', model)])\n",
        "                grid = GridSearchCV(\n",
        "                    pipeline, param_grid, scoring=product_pr_scorer,\n",
        "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), n_jobs=-1\n",
        "                )\n",
        "                grid.fit(X_train, y_train)\n",
        "                print(f\"Best parameters for {name}: {grid.best_params_}\")\n",
        "                model_optimized = grid.best_estimator_.named_steps['clf']\n",
        "\n",
        "            elif name == \"LDA\":\n",
        "                param_grid = {\n",
        "                    \"clf__solver\": [\"lsqr\", \"eigen\"],\n",
        "                    \"clf__shrinkage\": [0.3, 0.5, 0.7]\n",
        "                }\n",
        "                pipeline = Pipeline([('clf', model)])\n",
        "                grid = GridSearchCV(\n",
        "                    pipeline, param_grid, scoring=product_pr_scorer,\n",
        "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "                    n_jobs=-1, error_score='raise'\n",
        "                )\n",
        "                grid.fit(X_train, y_train)\n",
        "                print(f\"Best parameters for {name}: {grid.best_params_}\")\n",
        "                model_optimized = grid.best_estimator_.named_steps['clf']\n",
        "\n",
        "            else:\n",
        "                model_optimized = model\n",
        "\n",
        "            optimized_local[name] = model_optimized\n",
        "\n",
        "            # Learning Curve (Optimized)\n",
        "            print(f\"\\n--- Learning Curve for optimized {name} model ({method_name}) ---\")\n",
        "            plot_learning_curve(\n",
        "                model_optimized,\n",
        "                f\"Optimized {name} Learning Curve ({method_name})\",\n",
        "                X_train, y_train,\n",
        "                cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "                scoring=scoring_metric\n",
        "            )\n",
        "\n",
        "            # Post-Optimized Train vs. Test Error (Using Log Loss)\n",
        "            print(f\"\\n--- Post-Optimized {name} Train vs. Test Loss ({method_name}) ---\")\n",
        "            model_optimized.fit(X_train, y_train)\n",
        "            plot_train_test_error(\n",
        "                model_optimized, X_train, y_train, X_test, y_test,\n",
        "                model_name=f\"{name}_PostOptim\", method_name=method_name,\n",
        "                scoring=scoring_metric\n",
        "            )\n",
        "\n",
        "            # Post-Optimized KFold CV Accuracy\n",
        "            print(f\"\\n--- Post-Optimized KFold CV Accuracy for {name} ({method_name}) ---\")\n",
        "            cv_post = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "            post_fold_accuracies = []\n",
        "            for fold, (train_idx, val_idx) in enumerate(cv_post.split(X_train, y_train), 1):\n",
        "                X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "                y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "                model_optimized.fit(X_fold_train, y_fold_train)\n",
        "                acc = model_optimized.score(X_fold_val, y_fold_val)\n",
        "                post_fold_accuracies.append(acc)\n",
        "                print(f\"{name} (Post-Optimized) - Fold {fold} Accuracy: {acc:.4f}\")\n",
        "            mean_post_acc = np.mean(post_fold_accuracies)\n",
        "            print(f\"{name} (Post-Optimized) - Mean CV Accuracy: {mean_post_acc:.4f}\")\n",
        "            plt.figure(figsize=(6,4))\n",
        "            plt.bar(range(1, len(post_fold_accuracies)+1), post_fold_accuracies, color=\"#2ca02c\")\n",
        "            plt.title(f\"Post-Optimized CV Accuracy per Fold for {name}\")\n",
        "            plt.xlabel(\"Fold Number\")\n",
        "            plt.ylabel(\"Accuracy\")\n",
        "            plt.ylim(0, 1)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Evaluate on Test Set: Unadjusted and Adjusted Confusion Matrices\n",
        "            if hasattr(model_optimized, \"predict_proba\"):\n",
        "                # Compute unadjusted predictions and confusion matrix.\n",
        "                y_pred = model_optimized.predict(X_test)\n",
        "                cm_unadjusted = confusion_matrix(y_test, y_pred)\n",
        "                report_unadjusted = classification_report(y_test, y_pred, zero_division=0, output_dict=True)\n",
        "                print(f\"\\nUnadjusted Classification Report for {name} ({method_name}):\")\n",
        "                print(classification_report(y_test, y_pred, zero_division=0))\n",
        "                plot_confusion_matrix(cm_unadjusted, f\"Unadjusted Confusion Matrix: {name} ({method_name})\")\n",
        "\n",
        "                # Now apply threshold adjustment using precision-recall.\n",
        "                y_scores = model_optimized.predict_proba(X_test)[:, 1]\n",
        "                best_threshold = plot_precision_recall_and_threshold(\n",
        "                    y_test, y_scores, model_name=name, method_name=method_name\n",
        "                )\n",
        "                y_pred_thresh = (y_scores >= best_threshold).astype(int)\n",
        "                report_adjusted = classification_report(y_test, y_pred_thresh, zero_division=0, output_dict=True)\n",
        "                print(f\"\\nAdjusted Classification Report at threshold {best_threshold:.2f} for {name} ({method_name}):\")\n",
        "                print(classification_report(y_test, y_pred_thresh, zero_division=0))\n",
        "                cm_adjusted = confusion_matrix(y_test, y_pred_thresh)\n",
        "                plot_confusion_matrix(cm_adjusted, f\"Final Adjusted Confusion Matrix: {name} ({method_name})\")\n",
        "                results_local[name] = {\n",
        "                    \"confusion_matrix_unadjusted\": cm_unadjusted,\n",
        "                    \"classification_report_unadjusted\": report_unadjusted,\n",
        "                    \"confusion_matrix_adjusted\": cm_adjusted,\n",
        "                    \"classification_report_adjusted\": report_adjusted,\n",
        "                    \"FeatureSelectionMethod\": method_name\n",
        "                }\n",
        "            else:\n",
        "                y_pred = model_optimized.predict(X_test)\n",
        "                cm = confusion_matrix(y_test, y_pred)\n",
        "                report = classification_report(y_test, y_pred, zero_division=0, output_dict=True)\n",
        "                plot_confusion_matrix(cm, f\"Final Confusion Matrix: {name} ({method_name})\")\n",
        "                results_local[name] = {\n",
        "                    \"confusion_matrix\": cm,\n",
        "                    \"classification_report_unadjusted\": report,\n",
        "                    \"FeatureSelectionMethod\": method_name\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating {name} on {method_name}: {e}\")\n",
        "            results_local[name] = {\"error\": str(e), \"FeatureSelectionMethod\": method_name}\n",
        "\n",
        "    return {\"performance\": results_local, \"optimized_models\": optimized_local}\n",
        "\n",
        "\n",
        "# === STEP 3: Run Evaluations on Each Feature Selection Method ===\n",
        "evaluation_results = {}\n",
        "for method_key, df in feature_selection_methods.items():\n",
        "    evaluation_results[method_key] = train_and_evaluate_models(df, method_key)\n",
        "\n",
        "print(\"\\n=== Model Evaluation Completed for All Feature Selection Methods ===\")\n",
        "\n",
        "# Save evaluation results\n",
        "joblib.dump(evaluation_results, 'evaluation_results.pkl')\n",
        "print(\"Evaluation results saved to 'evaluation_results.pkl'\")\n",
        "\n",
        "optimized_models = {}\n",
        "for method_key, result in evaluation_results.items():\n",
        "    optimized_models[method_key] = result.get(\"optimized_models\", {})\n",
        "joblib.dump(optimized_models, 'optimized_models.pkl')\n",
        "print(\"Optimized models saved to 'optimized_models.pkl'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsjNYDmgTj7a"
      },
      "outputs": [],
      "source": [
        "lol = \"extracted_features_with_prep.csv\"\n",
        "lol2 = pd.read_csv(lol)\n",
        "lol2.columns\n",
        "\n",
        "files.download(\"final_Standard_RFECV_LR.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBP6YfGNuDqr"
      },
      "source": [
        "Minimize diff grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxZ241XKuH9x"
      },
      "outputs": [],
      "source": [
        "# Suppress warnings for undefined metrics and deprecated messages.\n",
        "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\"`use_label_encoder` is deprecated\")\n",
        "\n",
        "\n",
        "# Maximizing Product of Precision & Recall while Minimizing Their Difference\n",
        "def composite_precision_recall(y_true, y_pred, lambda_val=1.5):\n",
        "    p = precision_score(y_true, y_pred, zero_division=0)\n",
        "    r = recall_score(y_true, y_pred, zero_division=0)\n",
        "    return (p + r) - lambda_val * abs(p - r)\n",
        "\n",
        "composite_scorer = make_scorer(composite_precision_recall, greater_is_better=True)\n",
        "\n",
        "# Updated Learning Curve Function using Training Loss (Log Loss)\n",
        "def plot_learning_curve(estimator, title, X, y, cv, scoring):\n",
        "    train_sizes, train_scores, val_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, scoring=scoring, n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Convert negative log loss to positive loss if needed.\n",
        "    if scoring == 'neg_log_loss':\n",
        "        train_loss = -np.mean(train_scores, axis=1)\n",
        "        val_loss = -np.mean(val_scores, axis=1)\n",
        "    else:\n",
        "        train_loss = 1 - np.mean(train_scores, axis=1)\n",
        "        val_loss = 1 - np.mean(val_scores, axis=1)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(train_sizes, train_loss, 'o-', color=\"r\", label=\"Training Loss\")\n",
        "    plt.plot(train_sizes, val_loss, 'o-', color=\"g\", label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Number of Training Samples\")\n",
        "    plt.ylabel(\"Log Loss\")\n",
        "    plt.title(title)\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return train_sizes, train_loss, val_loss\n",
        "\n",
        "# Flatten nested dictionaries for CSV storage\n",
        "def flatten_dict(d, parent_key='', sep='_'):\n",
        "    items = []\n",
        "    for k, v in d.items():\n",
        "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
        "        if isinstance(v, dict):\n",
        "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
        "        else:\n",
        "            items.append((new_key, v))\n",
        "    return dict(items)\n",
        "\n",
        "\n",
        "# Train-vs-Test Error Function Using Log Loss\n",
        "def plot_train_test_error(model, X_train, y_train, X_test, y_test, model_name, method_name, scoring=\"neg_log_loss\"):\n",
        "\n",
        "    try:\n",
        "        y_train_pred_prob = model.predict_proba(X_train)\n",
        "        y_test_pred_prob  = model.predict_proba(X_test)\n",
        "    except Exception as e:\n",
        "        print(\"Model does not support predict_proba, falling back to dummy conversion.\")\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        y_test_pred  = model.predict(X_test)\n",
        "        y_train_pred_prob = np.vstack([1 - y_train_pred, y_train_pred]).T\n",
        "        y_test_pred_prob  = np.vstack([1 - y_test_pred, y_test_pred]).T\n",
        "\n",
        "    train_loss = log_loss(y_train, y_train_pred_prob)\n",
        "    test_loss  = log_loss(y_test, y_test_pred_prob)\n",
        "    metric_label = \"Log Loss\"\n",
        "\n",
        "    loss_difference = test_loss - train_loss\n",
        "    loss_ratio = test_loss / train_loss if train_loss != 0 else float('inf')\n",
        "\n",
        "    print(f\"{model_name} ({method_name}):\")\n",
        "    print(f\"  Training Loss: {train_loss:.4f}\")\n",
        "    print(f\"  Testing Loss:  {test_loss:.4f}\")\n",
        "    print(f\"  Loss Difference (Test - Train): {loss_difference:.4f}\")\n",
        "    print(f\"  Loss Ratio (Test / Train): {loss_ratio:.2f}\")\n",
        "\n",
        "    if loss_ratio > 1.2:\n",
        "        print(\"  Warning: Test Loss is significantly higher than Training Loss. Possible overfitting detected.\")\n",
        "    else:\n",
        "        print(\"  Training and Testing Loss are similar. Overfitting is less likely.\")\n",
        "\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.bar([\"Train Loss\", \"Test Loss\"], [train_loss, test_loss], color=[\"#1f77b4\", \"#ff7f0e\"])\n",
        "    plt.title(f\"Training vs. Testing Loss\\n{model_name} ({method_name})\")\n",
        "    plt.ylabel(metric_label)\n",
        "    plt.grid(axis='y', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Finding the Threshold Where Precision - Recall is Minimized\n",
        "def plot_precision_recall_and_threshold(y_true, y_scores, model_name, method_name):\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
        "    # Compute F1 scores for each threshold\n",
        "    f1_scores = 2 * precision[:-1] * recall[:-1] / (precision[:-1] + recall[:-1] + 1e-10)\n",
        "    best_idx = np.argmax(f1_scores)\n",
        "    best_threshold = thresholds[best_idx]\n",
        "    print(f\"Optimal threshold for {model_name} ({method_name}) based on max F1 score: {best_threshold:.2f}\")\n",
        "    avg_precision = average_precision_score(y_true, y_scores)\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.plot(recall, precision, label=f'AP = {avg_precision:.2f}')\n",
        "    plt.scatter(recall[best_idx], precision[best_idx], color='red',\n",
        "                label=f'Optimal (F1 = {f1_scores[best_idx]:.2f})')\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.title(f\"Precision-Recall Curve: {model_name} ({method_name})\\n(Maximizing F1 Score)\")\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return best_threshold\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load Feature Selection Outputs\n",
        "print(\"=== Step 1: Load Feature Selection Outputs ===\")\n",
        "scalers = [\"Standard\", \"MinMax\", \"Robust\"]\n",
        "methods = [\"RFECV_LR\", \"PCA\", \"MutualInfo\", \"Full\"]\n",
        "\n",
        "feature_selection_methods = {}\n",
        "for scaler in scalers:\n",
        "    for method in methods:\n",
        "        key = f\"{scaler}_{method}\"\n",
        "        filename = f\"final_{key}.csv\"\n",
        "        try:\n",
        "            df = pd.read_csv(filename)\n",
        "            feature_selection_methods[key] = df\n",
        "            print(f\"Loaded {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {filename}: {e}\")\n",
        "\n",
        "\n",
        "# Define Models\n",
        "def define_models():\n",
        "    return {\n",
        "        \"SVM\": SVC(kernel='linear', probability=True, random_state=42, max_iter=5000),\n",
        "        \"SVM_RBF\": SVC(kernel='rbf', probability=True, random_state=42, max_iter=5000),\n",
        "        \"SVM_Poly\": SVC(kernel='poly', probability=True, random_state=42, max_iter=5000),\n",
        "        \"L1LogisticRegression\": LogisticRegression(penalty='l1', solver='liblinear',\n",
        "                                                   class_weight='balanced', random_state=42, max_iter=1000),\n",
        "        \"LDA\": LinearDiscriminantAnalysis()\n",
        "    }\n",
        "\n",
        "# Set the grid search scoring metric to maximize\n",
        "grid_search_scoring = \"f1\"\n",
        "\n",
        "\n",
        "# Train & Evaluate Models\n",
        "def train_and_evaluate_models(X_df, method_name):\n",
        "    print(f\"\\n\\033[1;34m=== TESTING FEATURE SELECTION METHOD: {method_name} ===\\033[0m\")\n",
        "\n",
        "    # PARTIAL UNDERSAMPLING\n",
        "    df_combined = X_df.copy()\n",
        "    counts = df_combined['Status'].value_counts()\n",
        "    if len(counts) == 2:\n",
        "        minority_class = counts.idxmin()\n",
        "        majority_class = counts.idxmax()\n",
        "        minority_count = counts.min()\n",
        "        desired_ratio = 2\n",
        "        df_minority = df_combined[df_combined['Status'] == minority_class]\n",
        "        df_majority = df_combined[df_combined['Status'] == majority_class]\n",
        "        sample_size = min(len(df_majority), desired_ratio * minority_count)\n",
        "        df_majority_sampled = df_majority.sample(sample_size, random_state=42)\n",
        "        df_undersampled = pd.concat([df_minority, df_majority_sampled])\n",
        "    else:\n",
        "        df_undersampled = df_combined\n",
        "    print(\"Data shape after partial undersampling:\", df_undersampled.shape)\n",
        "\n",
        "    # Separate features and target\n",
        "    X_features = df_undersampled.drop(columns=['Status'])\n",
        "    y_target = df_undersampled['Status']\n",
        "\n",
        "    # Train-Test Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_features, y_target, test_size=0.2, random_state=42, stratify=y_target\n",
        "    )\n",
        "    print(\"Training set shape:\", X_train.shape)\n",
        "    print(\"Testing set shape:\", X_test.shape)\n",
        "\n",
        "    models = define_models()\n",
        "    results_local = {}\n",
        "    optimized_local = {}\n",
        "\n",
        "    def plot_confusion_matrix(cm, title):\n",
        "        plt.figure(figsize=(6,4))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "        plt.title(title)\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def get_scoring(estimator):\n",
        "        return 'neg_log_loss'\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\n=== Training and Evaluating Model: {name} ({method_name}) ===\")\n",
        "        scoring_metric = get_scoring(model)\n",
        "\n",
        "        # Learning Curve (Pre-Optimized)\n",
        "        print(f\"\\n--- Learning Curve for initial {name} model ({method_name}) ---\")\n",
        "        plot_learning_curve(\n",
        "            model,\n",
        "            f\"Initial {name} Learning Curve ({method_name})\",\n",
        "            X_train, y_train,\n",
        "            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "            scoring=scoring_metric\n",
        "        )\n",
        "\n",
        "        # Pre-Optimized Train vs. Test Error (Using Log Loss)\n",
        "        print(f\"\\n--- Pre-Optimized {name} Train vs. Test Error ({method_name}) ---\")\n",
        "        try:\n",
        "            model.fit(X_train, y_train)\n",
        "            plot_train_test_error(\n",
        "                model, X_train, y_train, X_test, y_test,\n",
        "                model_name=f\"{name}_PreOptim\", method_name=method_name,\n",
        "                scoring=scoring_metric\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error fitting pre-optimized {name}: {e}\")\n",
        "\n",
        "        # Pre-Optimized KFold CV Accuracy\n",
        "        print(f\"\\n--- Pre-Optimized KFold CV Accuracy for {name} ({method_name}) ---\")\n",
        "        cv_pre = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        pre_fold_accuracies = []\n",
        "        for fold, (train_idx, val_idx) in enumerate(cv_pre.split(X_train, y_train), 1):\n",
        "            X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "            y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "            model.fit(X_fold_train, y_fold_train)\n",
        "            acc = model.score(X_fold_val, y_fold_val)\n",
        "            pre_fold_accuracies.append(acc)\n",
        "            print(f\"{name} (Pre-Optimized) - Fold {fold} Accuracy: {acc:.4f}\")\n",
        "        mean_pre_acc = np.mean(pre_fold_accuracies)\n",
        "        print(f\"{name} (Pre-Optimized) - Mean CV Accuracy: {mean_pre_acc:.4f}\")\n",
        "        plt.figure(figsize=(6,4))\n",
        "        plt.bar(range(1, len(pre_fold_accuracies)+1), pre_fold_accuracies, color=\"#2ca02c\")\n",
        "        plt.title(f\"Pre-Optimized CV Accuracy per Fold for {name}\")\n",
        "        plt.xlabel(\"Fold Number\")\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        plt.ylim(0, 1)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Grid Search using the Composite Precision-Recall Scorer\n",
        "        try:\n",
        "            if name == \"L1LogisticRegression\":\n",
        "                param_grid = {\"clf__class_weight\": [{0: 1, 1: w} for w in [2, 3, 4, 5, 6]]}\n",
        "                pipeline = Pipeline([('clf', model)])\n",
        "                grid = GridSearchCV(\n",
        "                    pipeline, param_grid, scoring=composite_scorer,\n",
        "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), n_jobs=-1\n",
        "                )\n",
        "                grid.fit(X_train, y_train)\n",
        "                print(f\"Best parameters for {name}: {grid.best_params_}\")\n",
        "                model_optimized = grid.best_estimator_.named_steps['clf']\n",
        "\n",
        "            elif name == \"RandomForest\":\n",
        "                param_grid = {\n",
        "                    \"clf__class_weight\": [{0: 1, 1: w} for w in [2, 3, 4, 5, 6]],\n",
        "                    \"clf__max_depth\": [None, 5, 10],\n",
        "                    \"clf__min_samples_leaf\": [1, 3, 5],\n",
        "                    \"clf__random_state\": [7, 42, 100, 2025]\n",
        "                }\n",
        "                pipeline = Pipeline([('clf', model)])\n",
        "                grid = GridSearchCV(\n",
        "                    pipeline, param_grid, scoring=composite_scorer,\n",
        "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), n_jobs=-1\n",
        "                )\n",
        "                grid.fit(X_train, y_train)\n",
        "                print(f\"Best parameters for {name}: {grid.best_params_}\")\n",
        "                model_optimized = grid.best_estimator_.named_steps['clf']\n",
        "\n",
        "            elif name in [\"SVM\", \"SVM_RBF\"]:\n",
        "                param_grid = {\"clf__C\": [0.01, 0.1, 1, 10, 100]}\n",
        "                if name == \"SVM_RBF\":\n",
        "                    param_grid[\"clf__gamma\"] = ['scale', 'auto']\n",
        "                pipeline = Pipeline([('clf', model)])\n",
        "                grid = GridSearchCV(\n",
        "                    pipeline, param_grid, scoring=composite_scorer,\n",
        "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), n_jobs=-1\n",
        "                )\n",
        "                grid.fit(X_train, y_train)\n",
        "                print(f\"Best parameters for {name}: {grid.best_params_}\")\n",
        "                model_optimized = grid.best_estimator_.named_steps['clf']\n",
        "\n",
        "            elif name == \"SVM_Poly\":\n",
        "                param_grid = {\"clf__C\": [0.01, 0.1, 1, 10, 100],\n",
        "                              \"clf__degree\": [2, 3, 4]}\n",
        "                pipeline = Pipeline([('clf', model)])\n",
        "                grid = GridSearchCV(\n",
        "                    pipeline, param_grid, scoring=composite_scorer,\n",
        "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), n_jobs=-1\n",
        "                )\n",
        "                grid.fit(X_train, y_train)\n",
        "                print(f\"Best parameters for {name}: {grid.best_params_}\")\n",
        "                model_optimized = grid.best_estimator_.named_steps['clf']\n",
        "\n",
        "            elif name == \"XGBoost\":\n",
        "                param_grid = {\n",
        "                    \"clf__learning_rate\": [0.01, 0.1, 0.2],\n",
        "                    \"clf__max_depth\": [3, 5, 7]\n",
        "                }\n",
        "                pipeline = Pipeline([('clf', model)])\n",
        "                grid = GridSearchCV(\n",
        "                    pipeline, param_grid, scoring=composite_scorer,\n",
        "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), n_jobs=-1\n",
        "                )\n",
        "                grid.fit(X_train, y_train)\n",
        "                print(f\"Best parameters for {name}: {grid.best_params_}\")\n",
        "                model_optimized = grid.best_estimator_.named_steps['clf']\n",
        "\n",
        "            elif name == \"LDA\":\n",
        "                param_grid = {\n",
        "                    \"clf__solver\": [\"lsqr\", \"eigen\"],\n",
        "                    \"clf__shrinkage\": [0.3, 0.5, 0.7]\n",
        "                }\n",
        "                pipeline = Pipeline([('clf', model)])\n",
        "                grid = GridSearchCV(\n",
        "                    pipeline, param_grid, scoring=composite_scorer,\n",
        "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "                    n_jobs=-1, error_score='raise'\n",
        "                )\n",
        "                grid.fit(X_train, y_train)\n",
        "                print(f\"Best parameters for {name}: {grid.best_params_}\")\n",
        "                model_optimized = grid.best_estimator_.named_steps['clf']\n",
        "\n",
        "            else:\n",
        "                model_optimized = model\n",
        "\n",
        "            optimized_local[name] = model_optimized\n",
        "\n",
        "            # Learning Curve (Optimized)\n",
        "            print(f\"\\n--- Learning Curve for optimized {name} model ({method_name}) ---\")\n",
        "            plot_learning_curve(\n",
        "                model_optimized,\n",
        "                f\"Optimized {name} Learning Curve ({method_name})\",\n",
        "                X_train, y_train,\n",
        "                cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "                scoring=scoring_metric\n",
        "            )\n",
        "\n",
        "            # Post-Optimized Train vs. Test Error (Using Log Loss)\n",
        "            print(f\"\\n--- Post-Optimized {name} Train vs. Test Loss ({method_name}) ---\")\n",
        "            model_optimized.fit(X_train, y_train)\n",
        "            plot_train_test_error(\n",
        "                model_optimized, X_train, y_train, X_test, y_test,\n",
        "                model_name=f\"{name}_PostOptim\", method_name=method_name,\n",
        "                scoring=scoring_metric\n",
        "            )\n",
        "\n",
        "            # Post-Optimized KFold CV Accuracy\n",
        "            print(f\"\\n--- Post-Optimized KFold CV Accuracy for {name} ({method_name}) ---\")\n",
        "            cv_post = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "            post_fold_accuracies = []\n",
        "            for fold, (train_idx, val_idx) in enumerate(cv_post.split(X_train, y_train), 1):\n",
        "                X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "                y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "                model_optimized.fit(X_fold_train, y_fold_train)\n",
        "                acc = model_optimized.score(X_fold_val, y_fold_val)\n",
        "                post_fold_accuracies.append(acc)\n",
        "                print(f\"{name} (Post-Optimized) - Fold {fold} Accuracy: {acc:.4f}\")\n",
        "            mean_post_acc = np.mean(post_fold_accuracies)\n",
        "            print(f\"{name} (Post-Optimized) - Mean CV Accuracy: {mean_post_acc:.4f}\")\n",
        "            plt.figure(figsize=(6,4))\n",
        "            plt.bar(range(1, len(post_fold_accuracies)+1), post_fold_accuracies, color=\"#2ca02c\")\n",
        "            plt.title(f\"Post-Optimized CV Accuracy per Fold for {name}\")\n",
        "            plt.xlabel(\"Fold Number\")\n",
        "            plt.ylabel(\"Accuracy\")\n",
        "            plt.ylim(0, 1)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Unadjusted and Adjusted Confusion Matrices\n",
        "            if hasattr(model_optimized, \"predict_proba\"):\n",
        "                # Compute unadjusted predictions and confusion matrix.\n",
        "                y_pred = model_optimized.predict(X_test)\n",
        "                cm_unadjusted = confusion_matrix(y_test, y_pred)\n",
        "                report_unadjusted = classification_report(y_test, y_pred, zero_division=0, output_dict=True)\n",
        "                print(f\"\\nUnadjusted Classification Report for {name} ({method_name}):\")\n",
        "                print(classification_report(y_test, y_pred, zero_division=0))\n",
        "                plot_confusion_matrix(cm_unadjusted, f\"Unadjusted Confusion Matrix: {name} ({method_name})\")\n",
        "\n",
        "                # Compute adjusted predictions based on precision-recall threshold.\n",
        "                y_scores = model_optimized.predict_proba(X_test)[:, 1]\n",
        "                best_threshold = plot_precision_recall_and_threshold(\n",
        "                    y_test, y_scores, model_name=name, method_name=method_name\n",
        "                )\n",
        "                y_pred_thresh = (y_scores >= best_threshold).astype(int)\n",
        "                report_adjusted = classification_report(y_test, y_pred_thresh, zero_division=0, output_dict=True)\n",
        "                print(f\"\\nAdjusted Classification Report at threshold {best_threshold:.2f} for {name} ({method_name}):\")\n",
        "                print(classification_report(y_test, y_pred_thresh, zero_division=0))\n",
        "                cm_adjusted = confusion_matrix(y_test, y_pred_thresh)\n",
        "                plot_confusion_matrix(cm_adjusted, f\"Final Adjusted Confusion Matrix: {name} ({method_name})\")\n",
        "                results_local[name] = {\n",
        "                    \"confusion_matrix_unadjusted\": cm_unadjusted,\n",
        "                    \"classification_report_unadjusted\": report_unadjusted,\n",
        "                    \"confusion_matrix_adjusted\": cm_adjusted,\n",
        "                    \"classification_report_adjusted\": report_adjusted,\n",
        "                    \"FeatureSelectionMethod\": method_name\n",
        "                }\n",
        "            else:\n",
        "                y_pred = model_optimized.predict(X_test)\n",
        "                cm = confusion_matrix(y_test, y_pred)\n",
        "                report = classification_report(y_test, y_pred, zero_division=0, output_dict=True)\n",
        "                plot_confusion_matrix(cm, f\"Final Confusion Matrix: {name} ({method_name})\")\n",
        "                results_local[name] = {\n",
        "                    \"confusion_matrix\": cm,\n",
        "                    \"classification_report_unadjusted\": report,\n",
        "                    \"FeatureSelectionMethod\": method_name\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating {name} on {method_name}: {e}\")\n",
        "            results_local[name] = {\"error\": str(e), \"FeatureSelectionMethod\": method_name}\n",
        "\n",
        "    return {\"performance\": results_local, \"optimized_models\": optimized_local}\n",
        "\n",
        "\n",
        "# Run Evaluations on Each Feature Selection Method\n",
        "evaluation_results = {}\n",
        "for method_key, df in feature_selection_methods.items():\n",
        "    evaluation_results[method_key] = train_and_evaluate_models(df, method_key)\n",
        "\n",
        "print(\"\\n=== Model Evaluation Completed for All Feature Selection Methods ===\")\n",
        "\n",
        "# Save evaluation results (performance metrics and optimized models) for later use.\n",
        "joblib.dump(evaluation_results, 'evaluation_results.pkl')\n",
        "print(\"Evaluation results saved to 'evaluation_results.pkl'\")\n",
        "\n",
        "optimized_models = {}\n",
        "for method_key, result in evaluation_results.items():\n",
        "    optimized_models[method_key] = result.get(\"optimized_models\", {})\n",
        "joblib.dump(optimized_models, 'optimized_models.pkl')\n",
        "print(\"Optimized models saved to 'optimized_models.pkl'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZKo5OZVTr4d"
      },
      "outputs": [],
      "source": [
        "lol = \"final_Standard_Allowed.csv\"\n",
        "lol2 = pd.read_csv(lol)\n",
        "lol2.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfY1AxLwQIpI"
      },
      "source": [
        "Characterizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0v-1J9uUV6Z"
      },
      "source": [
        "sources:\n",
        "\n",
        "Man whitney for ranking:\n",
        "https://pubmed.ncbi.nlm.nih.gov/25555756/ </br>\n",
        "https://pmc.ncbi.nlm.nih.gov/articles/PMC10973702/?utm_source=chatgpt.com#hbm26555-sec-0026</br>\n",
        "https://pmc.ncbi.nlm.nih.gov/articles/PMC3536906/?utm_source=chatgpt.com#sec23</br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXbD5Ha_TVv4"
      },
      "outputs": [],
      "source": [
        "# Load the feature ranking analysis file\n",
        "analysis_file = \"final_Standard_Allowed_feature_analysis.csv\"\n",
        "features_df = pd.read_csv(analysis_file)\n",
        "\n",
        "# Select the relevant columns for in-depth normality and difference analysis:\n",
        "features_df = features_df[[\"Feature\", \"Test_PValue\", \"Test_Statistic\", \"Shapiro_PValue\"]]\n",
        "\n",
        "# Sort features by the Mann–Whitney p-value (lower p-value => stronger evidence of difference)\n",
        "sorted_by_diff = features_df.sort_values(by=\"Test_PValue\", ascending=True)\n",
        "sorted_by_diff['Decision'] = sorted_by_diff[\"Test_PValue\"].apply(\n",
        "    lambda x: \"Reject Null (Feature Differs)\" if x < 0.05 else \"Fail to Reject Null\"\n",
        ")\n",
        "\n",
        "# Sort features by the Mann–Whitney test statistic (higher statistic => stronger separation)\n",
        "sorted_by_separability = features_df.sort_values(by=\"Test_Statistic\", ascending=False)\n",
        "\n",
        "# Mann–Whitney U Test p-values\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.barh(sorted_by_diff[\"Feature\"], sorted_by_diff[\"Test_PValue\"], color='blue', alpha=0.7)\n",
        "plt.xlabel(\"Mann–Whitney U Test p-value\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Mann–Whitney U Test p-values\\n(Lower p-value indicates stronger evidence of group differences)\")\n",
        "# Add vertical line indicating p = 0.05 threshold\n",
        "plt.axvline(0.05, color='red', linestyle='--', label='p = 0.05')\n",
        "plt.legend()\n",
        "plt.gca().invert_yaxis()  # highest significance at top\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Mann–Whitney U Test Statistic\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.barh(sorted_by_separability[\"Feature\"], sorted_by_separability[\"Test_Statistic\"], color='green', alpha=0.7)\n",
        "plt.xlabel(\"Mann–Whitney U Test Statistic\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Mann–Whitney U Test Statistic\\n(Higher statistic indicates stronger group separability)\")\n",
        "plt.gca().invert_yaxis()  # highest statistic at top\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Shapiro–Wilk Normality Test p-values (with log scaling on the x-axis)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.barh(sorted_by_diff[\"Feature\"], sorted_by_diff[\"Shapiro_PValue\"], color='orange', alpha=0.7)\n",
        "plt.xlabel(\"Shapiro–Wilk Test p-value (Log Scale)\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Shapiro–Wilk Normality Test p-values\\n(Lower p-value indicates deviation from normality)\")\n",
        "plt.xscale(\"log\")\n",
        "\n",
        "# Add vertical line indicating p = 0.05 threshold\n",
        "plt.axvline(0.05, color='red', linestyle='--', label='p = 0.05')\n",
        "plt.legend()\n",
        "plt.gca().invert_yaxis()  # features with lowest p-value at top\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation Heatmap of Analysis Metrics\n",
        "plt.figure(figsize=(6, 4))\n",
        "numeric_cols = features_df.select_dtypes(include=[np.number]).columns\n",
        "sns.heatmap(features_df[numeric_cols].corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap of Analysis Metrics\\n(Mann–Whitney and Shapiro–Wilk)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Pairplot of Analysis Metrics\n",
        "sns.pairplot(features_df, diag_kind='kde')\n",
        "plt.suptitle(\"Pairplot of Analysis Metrics\\n(Mann–Whitney p-value, Test Statistic, Shapiro–Wilk p-value)\", y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# Scatter Plot of Mann–Whitney Metrics\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(features_df[\"Test_PValue\"], features_df[\"Test_Statistic\"],\n",
        "                      c=features_df[\"Shapiro_PValue\"], cmap='viridis', s=100, alpha=0.7)\n",
        "plt.xlabel(\"Mann–Whitney U Test p-value\")\n",
        "plt.ylabel(\"Mann–Whitney U Test Statistic\")\n",
        "plt.title(\"Scatter Plot: Mann–Whitney p-value vs. Test Statistic\\n(Colored by Shapiro–Wilk p-value)\")\n",
        "plt.colorbar(scatter, label=\"Shapiro–Wilk p-value\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Violin Plots of Analysis Metrics\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.violinplot(data=features_df[[\"Test_PValue\", \"Test_Statistic\", \"Shapiro_PValue\"]])\n",
        "plt.title(\"Violin Plots of Analysis Metrics\\n(Distribution of Mann–Whitney and Shapiro–Wilk p-values/test statistics)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display\n",
        "print(\"=== Sorted by Mann–Whitney p-value (Lowest p-value = strongest evidence of difference) ===\")\n",
        "display(sorted_by_diff)\n",
        "\n",
        "print(\"\\n=== Sorted by Mann–Whitney Test Statistic (Highest = strongest group separability) ===\")\n",
        "display(sorted_by_separability)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umKvNEg4cZPH"
      },
      "outputs": [],
      "source": [
        "lol = \"final_Standard_Allowed.csv\"\n",
        "lol2 = pd.read_csv(lol)\n",
        "lol2.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFTPkI7cvo2L"
      },
      "source": [
        "***Ranking with PCA***\n",
        "\n",
        "sources:<br>\n",
        "basics - https://arxiv.org/pdf/1404.1100 <br>\n",
        "ranking formula - https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0183997"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPdmnIdDPu1c"
      },
      "outputs": [],
      "source": [
        "filename = \"final_Robust_MutualInfo.csv\"\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.isfile(filename):\n",
        "    print(f\"File not found: {filename}\")\n",
        "else:\n",
        "    # Load the data\n",
        "    df = pd.read_csv(filename)\n",
        "\n",
        "    features = df.select_dtypes(include=[np.number])\n",
        "    if 'Status' in features.columns:\n",
        "        features = features.drop(columns=['Status'])\n",
        "\n",
        "    if features.empty:\n",
        "        print(f\"No numeric features to process in {filename}\")\n",
        "    else:\n",
        "\n",
        "        X_scaled = features\n",
        "        X_scaled = pd.DataFrame(X_scaled, columns=features.columns)\n",
        "\n",
        "        # PCA for top 90% variance\n",
        "        pca = PCA(n_components=0.90)\n",
        "        pca.fit(X_scaled)\n",
        "        n_components = pca.n_components_\n",
        "        print(f\"Number of PCA components selected to explain 90% variance: {n_components}\")\n",
        "\n",
        "        loadings = pd.DataFrame(\n",
        "            pca.components_.T,\n",
        "            index=features.columns,\n",
        "            columns=[f\"PC{i+1}\" for i in range(n_components)]\n",
        "        )\n",
        "\n",
        "        feature_importance = (loadings**2).sum(axis=1)\n",
        "        ranking = feature_importance.sort_values(ascending=False)\n",
        "\n",
        "        print(f\"\\nPCA Feature Ranking for {filename} (using top 90% variance components):\")\n",
        "        print(ranking)\n",
        "\n",
        "        # Plot cumulative explained variance (selected components)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        weights = pca.explained_variance_ratio_\n",
        "        cum_var = np.cumsum(weights)\n",
        "        plt.plot(range(1, n_components+1), cum_var, marker='o')\n",
        "        plt.xticks(range(1, n_components+1))\n",
        "        plt.xlabel(\"Principal Component\")\n",
        "        plt.ylabel(\"Cumulative Explained Variance\")\n",
        "        plt.title(f\"Cumulative Explained Variance (90%)\\n{filename}\")\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # PCA on all components to see full cumulative explained variance\n",
        "        pca_full = PCA()\n",
        "        pca_full.fit(X_scaled)\n",
        "\n",
        "        full_weights = pca_full.explained_variance_ratio_\n",
        "        full_cum_var = np.cumsum(full_weights)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(np.arange(1, len(full_cum_var)+1), full_cum_var, marker='o', color='purple')\n",
        "        plt.xticks(np.arange(1, len(full_cum_var)+1))\n",
        "        plt.xlabel(\"Principal Component\")\n",
        "        plt.ylabel(\"Cumulative Explained Variance\")\n",
        "        plt.title(f\"Cumulative Explained Variance (All Components)\\n{filename}\")\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Feature Contributions per selected Principal Component\n",
        "        for i in range(n_components):\n",
        "            pc = f\"PC{i+1}\"\n",
        "            pc_contrib = loadings[pc]**2\n",
        "            pc_contrib_sorted = pc_contrib.sort_values(ascending=True)\n",
        "\n",
        "            plt.figure(figsize=(10, max(5, 0.3 * len(pc_contrib_sorted))))\n",
        "            plt.barh(pc_contrib_sorted.index, pc_contrib_sorted.values, color='lightgreen')\n",
        "            plt.xlabel(\"Squared Loading (Variance Contribution)\")\n",
        "            plt.title(f\"Feature Contribution for {pc}\\n{filename}\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        # Total Variance Contribution by Feature (Communality)\n",
        "        plt.figure(figsize=(10, max(5, 0.3 * len(ranking))))\n",
        "        plt.barh(ranking.index, ranking.values, color='salmon')\n",
        "        plt.xlabel(\"Total Variance Contribution (Communality)\")\n",
        "        plt.title(f\"Total Variance Contribution by Feature (Top Components)\\n{filename}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # PCA ranking (communality scores)\n",
        "        ranking_sorted = ranking.sort_values(ascending=True)\n",
        "        plt.figure(figsize=(10, max(5, 0.3 * len(ranking_sorted))))\n",
        "        plt.barh(ranking_sorted.index, ranking_sorted.values, color='skyblue')\n",
        "        plt.xlabel(\"Loading Score (Communality)\")\n",
        "        plt.title(f\"PCA Feature Ranking\\n{filename}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT33jM68T3-2"
      },
      "outputs": [],
      "source": [
        "#files.download(\"final_Standard_RFECV_LR.csv\")\n",
        "\"\"\"files.download(\"final_Robust_RFECV_LR.csv\")\n",
        "files.download(\"final_Robust_RFECV_SVM.csv\")\"\"\"\n",
        "pd.read_csv(\"final_Standard_Allowed.csv\")\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8zHFmr8igjZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# Load your dataset.\n",
        "df = pd.read_csv('final_Robust_MutualInfo.csv')\n",
        "\n",
        "features = df.drop(columns=['Status'])\n",
        "target = df['Status']\n",
        "\n",
        "# Initialize a list to store the U test results.\n",
        "results = []\n",
        "\n",
        "# Loop over each feature, performing the Mann–Whitney U test between the two classes.\n",
        "for col in features.columns:\n",
        "    group0 = features[target == 0][col]\n",
        "    group1 = features[target == 1][col]\n",
        "    # Use a two-sided test.\n",
        "    U_statistic, p_value = mannwhitneyu(group0, group1, alternative='two-sided')\n",
        "    results.append({'Feature': col, 'U_statistic': U_statistic, 'p_value': p_value})\n",
        "\n",
        "# Convert results to a DataFrame and sort by p-value (lower is better).\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values(by='p_value')\n",
        "\n",
        "print(\"Feature ranking based on Mann–Whitney U test:\")\n",
        "print(results_df)\n",
        "\n",
        "# Plot the ranking of features by their p-value.\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='p_value', y='Feature', data=results_df, palette='viridis')\n",
        "plt.xlabel('Mann–Whitney U Test p-value')\n",
        "plt.title('Feature Ranking using U‑Filter (Mann–Whitney U Test)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcIMcaLHkaa-"
      },
      "outputs": [],
      "source": [
        "lol = \"final_Standard_Allowed.csv\"\n",
        "lol2 = pd.read_csv(lol)\n",
        "lol2.columns\n",
        "\n",
        "files.download(lol)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnwvmAJAlGT_"
      },
      "source": [
        "interclass correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBljS6balF9O"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"final_Robust_MutualInfo.csv\")\n",
        "\n",
        "features = df.drop(columns=[\"Status\"])\n",
        "target = df[\"Status\"]\n",
        "\n",
        "# Split the data into separate DataFrames for each class.\n",
        "df_class0 = df[df[\"Status\"] == 0].drop(columns=[\"Status\"])\n",
        "df_class1 = df[df[\"Status\"] == 1].drop(columns=[\"Status\"])\n",
        "\n",
        "# Compute the correlation matrix for each class.\n",
        "corr_class0 = df_class0.corr()\n",
        "corr_class1 = df_class1.corr()\n",
        "\n",
        "# Define annotation settings for clarity\n",
        "annot_kws = {\"fontsize\": 10}\n",
        "\n",
        "# Plot the correlation matrix for Class 0 with a larger figure.\n",
        "plt.figure(figsize=(16, 12))\n",
        "sns.heatmap(corr_class0, annot=True, fmt=\".2f\", cmap=\"coolwarm\", annot_kws=annot_kws)\n",
        "plt.title(\"Pearsons Correlation Matrix for Class 0 after feature selection\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot the correlation matrix for Class 1 with a larger figure.\n",
        "plt.figure(figsize=(16, 12))\n",
        "sns.heatmap(corr_class1, annot=True, fmt=\".2f\", cmap=\"coolwarm\", annot_kws=annot_kws)\n",
        "plt.title(\"Pearsons Correlation Matrix for Class 1 after feature selection\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compute the difference between the two correlation matrices.\n",
        "diff_corr = corr_class0 - corr_class1\n",
        "\n",
        "# Plot the difference in correlations with a larger figure.\n",
        "plt.figure(figsize=(16, 12))\n",
        "sns.heatmap(diff_corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", annot_kws=annot_kws)\n",
        "plt.title(\"Difference in Correlation (Class 0 - Class 1)\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDBehe-HkdSx"
      },
      "outputs": [],
      "source": [
        "lol = \"final_Standard_Allowed.csv\"\n",
        "lol2 = pd.read_csv(lol)\n",
        "lol2.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3Jo0acmo_yg"
      },
      "source": [
        "spearman on non normal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHSUzxWZuTz6"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"final_Robust_MutualInfo.csv\")\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51MkqSwqo_iB"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"final_Robust_MutualInfo.csv\")\n",
        "lol2 = ['Height', 'Active Playing Years', 'GrayLevelNonUniformity', 'Q-Angle',\n",
        "       'Mean', 'Gender', 'Dissimilarity_0', 'Homogeneity_0', 'Hu6', 'Status']\n",
        "non_normal_features = lol2\n",
        "\n",
        "df_class0 = df[df[\"Status\"] == 0][non_normal_features]\n",
        "df_class1 = df[df[\"Status\"] == 1][non_normal_features]\n",
        "\n",
        "corr_class0 = df_class0.corr(method=\"spearman\")\n",
        "corr_class1 = df_class1.corr(method=\"spearman\")\n",
        "\n",
        "annot_kws = {\"fontsize\": 10}\n",
        "\n",
        "plt.figure(figsize=(16, 12))\n",
        "sns.heatmap(corr_class0, annot=True, fmt=\".2f\", cmap=\"coolwarm\", annot_kws=annot_kws)\n",
        "plt.title(\"Spearman Correlation Matrix (Non-Normal) - Class 0\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(16, 12))\n",
        "sns.heatmap(corr_class1, annot=True, fmt=\".2f\", cmap=\"coolwarm\", annot_kws=annot_kws)\n",
        "plt.title(\"Spearman Correlation Matrix (Non-Normal) - Class 1\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "diff_corr = corr_class0 - corr_class1\n",
        "\n",
        "plt.figure(figsize=(16, 12))\n",
        "sns.heatmap(diff_corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", annot_kws=annot_kws)\n",
        "plt.title(\"Difference in Spearman Correlation (Class 0 - Class 1)\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_Np1zGFpC-s"
      },
      "outputs": [],
      "source": [
        "lol = \"final_Standard_RFECV_LR.csv\"\n",
        "lol2 = pd.read_csv(lol)\n",
        "lol2.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqNKt50tyZQ5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import ttest_ind\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('final_Standard_RFECV_LR.csv')\n",
        "\n",
        "if 'Status' not in df.columns:\n",
        "    raise ValueError(\"The dataset does not contain a 'Status' column.\")\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if 'Status' in numeric_cols:\n",
        "    numeric_cols.remove('Status')\n",
        "\n",
        "group0 = df[df['Status'] == 0]\n",
        "group1 = df[df['Status'] == 1]\n",
        "\n",
        "results = []\n",
        "for col in numeric_cols:\n",
        "    values0 = group0[col].dropna()\n",
        "    values1 = group1[col].dropna()\n",
        "    t_stat, p_value = ttest_ind(values0, values1, equal_var=False)\n",
        "    results.append({'Feature': col, 't_statistic': t_stat, 'p_value': p_value})\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values(by='p_value', ascending=True)\n",
        "\n",
        "print(\"Independent Samples t-test Results:\")\n",
        "print(results_df)\n",
        "\n",
        "# Plot p-values for visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=\"p_value\", y=\"Feature\", data=results_df, palette=\"viridis\")\n",
        "plt.axvline(0.05, color=\"red\", linestyle=\"--\", label=\"Significance Threshold (p=0.05)\")\n",
        "plt.xlabel(\"p-value\")\n",
        "plt.title(\"Independent Samples t-test p-values for each Feature\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYzV04D7Aqg5"
      },
      "outputs": [],
      "source": [
        "lol = \"final_Standard_RFECV_LR.csv\"\n",
        "lol2 = pd.read_csv(lol)\n",
        "lol2.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KccuBrwYvLF5"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv(\"final_Standard_RFECV_LR.csv\")\n",
        "\n",
        "# Ensure 'Status' column is present\n",
        "if 'Status' not in df.columns:\n",
        "    raise ValueError(\"The dataset does not contain a 'Status' column.\")\n",
        "\n",
        "# Identify numeric columns excluding 'Status'\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "numeric_cols.remove('Status')\n",
        "\n",
        "# Split the dataset into two groups based on the binary target\n",
        "group0 = df[df['Status'] == 0]\n",
        "group1 = df[df['Status'] == 1]\n",
        "\n",
        "# Conduct Welch's t-tests and store results\n",
        "results = []\n",
        "for col in numeric_cols:\n",
        "    values0 = group0[col].dropna()\n",
        "    values1 = group1[col].dropna()\n",
        "\n",
        "    t_stat, p_value = ttest_ind(values0, values1, equal_var=False)\n",
        "\n",
        "    results.append({'Feature': col, 't_statistic': t_stat, 'p_value': p_value})\n",
        "\n",
        "# Create DataFrame for results\n",
        "results_df = pd.DataFrame(results).sort_values(by='p_value')\n",
        "\n",
        "# Save results to CSV\n",
        "results_df.to_csv('ttest_results_selected_features.csv', index=False)\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.barplot(x=\"p_value\", y=\"Feature\", data=results_df, palette=\"viridis\")\n",
        "plt.axvline(0.05, color=\"red\", linestyle=\"--\", label=\"Significance Threshold (p=0.05)\")\n",
        "plt.xlabel(\"p-value\")\n",
        "plt.title(\"Feature Significance Based on Welch's t-test\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save results to CSV\n",
        "results_df.to_csv('ttest_results_RFECV_selected_features.csv', index=False)\n",
        "\n",
        "print(\"Results saved to 'ttest_results_export.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P03QWwnMvM0A"
      },
      "outputs": [],
      "source": [
        "lol = \"final_Standard_RFECV_LR.csv\"\n",
        "lol2 = pd.read_csv(lol)\n",
        "lol2.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23jy10OShWZB"
      },
      "outputs": [],
      "source": [
        "files.download(\"final_Standard_MutualInfo_feature_analysis.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-kELFTRefc7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}